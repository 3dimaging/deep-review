@article{3qm8sXnB,
 abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve
state-of-the-art performance on a variety of machine learning tasks. Several
researchers have recently proposed schemes to parallelize SGD, but all require
performance-destroying memory locking and synchronization. This work aims to
show using novel theoretical analysis, algorithms, and implementation that SGD
can be implemented without any locking. We present an update scheme called
HOGWILD! which allows processors access to shared memory with the possibility
of overwriting each other's work. We show that when the associated optimization
problem is sparse, meaning most gradient updates only modify small parts of the
decision variable, then HOGWILD! achieves a nearly optimal rate of convergence.
We demonstrate experimentally that HOGWILD! outperforms alternative schemes
that use locking by an order of magnitude.},
 archiveprefix = {arXiv},
 author = {Feng Niu and Benjamin Recht and Christopher Re and Stephen J. Wright},
 eprint = {1106.5730v2},
 file = {1106.5730v2.pdf},
 month = {Jun},
 primaryclass = {math.OC},
 title = {HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient
Descent},
 url = {https://arxiv.org/abs/1106.5730v2},
 year = {2011}
}


@article{8RAYEOPl,
 abstract = {We develop stochastic variational inference, a scalable algorithm for
approximating posterior distributions. We develop this technique for a large
class of probabilistic models and we demonstrate it with two probabilistic
topic models, latent Dirichlet allocation and the hierarchical Dirichlet
process topic model. Using stochastic variational inference, we analyze several
large collections of documents: 300K articles from Nature, 1.8M articles from
The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can
easily handle data sets of this size and outperforms traditional variational
inference, which can only handle a smaller subset. (We also show that the
Bayesian nonparametric topic model outperforms its parametric counterpart.)
Stochastic variational inference lets us apply complex Bayesian models to
massive data sets.},
 archiveprefix = {arXiv},
 author = {Matt Hoffman and David M. Blei and Chong Wang and John Paisley},
 eprint = {1206.7051v3},
 file = {1206.7051v3.pdf},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Stochastic Variational Inference},
 url = {https://arxiv.org/abs/1206.7051v3},
 year = {2012}
}


@article{g2vvbB91,
 abstract = {After a more than decade-long period of relatively little research activity
in the area of recurrent neural networks, several new developments will be
reviewed here that have allowed substantial progress both in understanding and
in technical solutions towards more efficient training of recurrent networks.
These advances have been motivated by and related to the optimization issues
surrounding deep learning. Although recurrent networks are extremely powerful
in what they can in principle represent in terms of modelling sequences,their
training is plagued by two aspects of the same issue regarding the learning of
long-term dependencies. Experiments reported here evaluate the use of clipping
gradients, spanning longer time ranges with leaky integration, advanced
momentum techniques, using more powerful output probability models, and
encouraging sparser gradients to help symmetry breaking and credit assignment.
The experiments are performed on text and music data and show off the combined
effects of these techniques in generally improving both training and test
error.},
 archiveprefix = {arXiv},
 author = {Yoshua Bengio and Nicolas Boulanger-Lewandowski and Razvan Pascanu},
 eprint = {1212.0901v2},
 file = {1212.0901v2.pdf},
 month = {12},
 primaryclass = {cs.LG},
 title = {Advances in Optimizing Recurrent Networks},
 url = {https://arxiv.org/abs/1212.0901v2},
 year = {2012}
}


@article{15y7iq6HF,
 abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be
used to generate complex sequences with long-range structure, simply by
predicting one data point at a time. The approach is demonstrated for text
(where the data are discrete) and online handwriting (where the data are
real-valued). It is then extended to handwriting synthesis by allowing the
network to condition its predictions on a text sequence. The resulting system
is able to generate highly realistic cursive handwriting in a wide variety of
styles.},
 archiveprefix = {arXiv},
 author = {Alex Graves},
 eprint = {1308.0850v5},
 file = {1308.0850v5.pdf},
 month = {Aug},
 primaryclass = {cs.NE},
 title = {Generating Sequences With Recurrent Neural Networks},
 url = {https://arxiv.org/abs/1308.0850v5},
 year = {2013}
}


@article{1Fel6Bdb8,
 abstract = {Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network's prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.},
 archiveprefix = {arXiv},
 author = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
 eprint = {1312.6199v4},
 file = {1312.6199v4.pdf},
 month = {12},
 primaryclass = {cs.CV},
 title = {Intriguing properties of neural networks},
 url = {https://arxiv.org/abs/1312.6199v4},
 year = {2013}
}


@article{8t43CQ9m,
 abstract = {Predicting protein secondary structure is a fundamental problem in protein
structure prediction. Here we present a new supervised generative stochastic
network (GSN) based method to predict local secondary structure with deep
hierarchical representations. GSN is a recently proposed deep learning
technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative
model. We present the supervised extension of GSN, which learns a Markov chain
to sample from a conditional distribution, and applied it to protein structure
prediction. To scale the model to full-sized, high-dimensional data, like
protein sequences with hundreds of amino acids, we introduce a convolutional
architecture, which allows efficient learning across multiple layers of
hierarchical representations. Our architecture uniquely focuses on predicting
structured low-level labels informed with both low and high-level
representations learned by the model. In our application this corresponds to
labeling the secondary structure state of each amino-acid residue. We trained
and tested the model on separate sets of non-homologous proteins sharing less
than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513
dataset, better than the previously reported best performance 64.9% (Wang et
al., 2011) for this challenging secondary structure prediction problem.},
 archiveprefix = {arXiv},
 author = {Jian Zhou and Olga G. Troyanskaya},
 eprint = {1403.1347v1},
 file = {1403.1347v1.pdf},
 month = {Mar},
 primaryclass = {q-bio.QM},
 title = {Deep Supervised and Convolutional Generative Stochastic Network for
Protein Secondary Structure Prediction},
 url = {https://arxiv.org/abs/1403.1347v1},
 year = {2014}
}


@article{pxdeuhMS,
 abstract = {We describe \textit{deep exponential families} (DEFs), a class of latent
variable models that are inspired by the hidden structures used in deep neural
networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We
perform inference using recent "black box" variational inference techniques. We
then evaluate various DEFs on text and combine multiple DEFs into a model for
pairwise recommendation data. In an extensive study, we show that going beyond
one layer improves predictions for DEFs. We demonstrate that DEFs find
interesting exploratory structure in large data sets, and give better
predictive performance than state-of-the-art models.},
 archiveprefix = {arXiv},
 author = {Rajesh Ranganath and Linpeng Tang and Laurent Charlin and David M. Blei},
 eprint = {1411.2581v1},
 file = {1411.2581v1.pdf},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Deep Exponential Families},
 url = {https://arxiv.org/abs/1411.2581v1},
 year = {2014}
}


@article{UtcyntjF,
 abstract = {Several machine learning models, including neural networks, consistently
misclassify adversarial examples---inputs formed by applying small but
intentionally worst-case perturbations to examples from the dataset, such that
the perturbed input results in the model outputting an incorrect answer with
high confidence. Early attempts at explaining this phenomenon focused on
nonlinearity and overfitting. We argue instead that the primary cause of neural
networks' vulnerability to adversarial perturbation is their linear nature.
This explanation is supported by new quantitative results while giving the
first explanation of the most intriguing fact about them: their generalization
across architectures and training sets. Moreover, this view yields a simple and
fast method of generating adversarial examples. Using this approach to provide
examples for adversarial training, we reduce the test set error of a maxout
network on the MNIST dataset.},
 archiveprefix = {arXiv},
 author = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
 eprint = {1412.6572v3},
 file = {1412.6572v3.pdf},
 month = {12},
 primaryclass = {stat.ML},
 title = {Explaining and Harnessing Adversarial Examples},
 url = {https://arxiv.org/abs/1412.6572v3},
 year = {2014}
}


@article{Z7fd0BYf,
 abstract = {Deep convolutional neural networks comprise a subclass of deep neural
networks (DNN) with a constrained architecture that leverages the spatial and
temporal structure of the domain they model. Convolutional networks achieve the
best predictive performance in areas such as speech and image recognition by
hierarchically composing simple local features into complex models. Although
DNNs have been used in drug discovery for QSAR and ligand-based bioactivity
predictions, none of these models have benefited from this powerful
convolutional architecture. This paper introduces AtomNet, the first
structure-based, deep convolutional neural network designed to predict the
bioactivity of small molecules for drug discovery applications. We demonstrate
how to apply the convolutional concepts of feature locality and hierarchical
composition to the modeling of bioactivity and chemical interactions. In
further contrast to existing DNN techniques, we show that AtomNet's application
of local convolutional filters to structural target information successfully
predicts new active molecules for targets with no previously known modulators.
Finally, we show that AtomNet outperforms previous docking approaches on a
diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9
on 57.8% of the targets in the DUDE benchmark.},
 archiveprefix = {arXiv},
 author = {Izhar Wallach and Michael Dzamba and Abraham Heifets},
 eprint = {1510.02855v1},
 file = {1510.02855v1.pdf},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction
in Structure-based Drug Discovery},
 url = {https://arxiv.org/abs/1510.02855v1},
 year = {2015}
}


@article{15lbUf0as,
 abstract = {Black box variational inference allows researchers to easily prototype and
evaluate an array of models. Recent advances allow such algorithms to scale to
high dimensions. However, a central question remains: How to specify an
expressive variational distribution that maintains efficient computation? To
address this, we develop hierarchical variational models (HVMs). HVMs augment a
variational approximation with a prior on its parameters, which allows it to
capture complex structure for both discrete and continuous latent variables.
The algorithm we develop is black box, can be used for any HVM, and has the
same computational efficiency as the original approximation. We study HVMs on a
variety of deep discrete latent variable models. HVMs generalize other
expressive variational distributions and maintains higher fidelity to the
posterior.},
 archiveprefix = {arXiv},
 author = {Rajesh Ranganath and Dustin Tran and David M. Blei},
 eprint = {1511.02386v2},
 file = {1511.02386v2.pdf},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Hierarchical Variational Models},
 url = {https://arxiv.org/abs/1511.02386v2},
 year = {2015}
}


@article{HRXii6Ni,
 abstract = {Personalized predictive medicine necessitates the modeling of patient illness
and care processes, which inherently have long-term temporal dependencies.
Healthcare observations, recorded in electronic medical records, are episodic
and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural
network that reads medical records, stores previous illness history, infers
current illness states and predicts future medical outcomes. At the data level, DeepCare represents care episodes as vectors in space, models patient health
state trajectories through explicit memory of historical records. Built on Long
Short-Term Memory (LSTM), DeepCare introduces time parameterizations to handle
irregular timed events by moderating the forgetting and consolidation of memory
cells. DeepCare also incorporates medical interventions that change the course
of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale
temporal pooling, before passing through a neural network that estimates future
outcomes. We demonstrate the efficacy of DeepCare for disease progression
modeling, intervention recommendation, and future risk prediction. On two
important cohorts with heavy social and economic burden -- diabetes and mental
health -- the results show improved modeling and risk prediction accuracy.},
 archiveprefix = {arXiv},
 author = {Trang Pham and Truyen Tran and Dinh Phung and Svetha Venkatesh},
 eprint = {1602.00357v2},
 file = {1602.00357v2.pdf},
 month = {Feb},
 primaryclass = {stat.ML},
 title = {DeepCare: A Deep Dynamic Memory Model for Predictive Medicine},
 url = {https://arxiv.org/abs/1602.00357v2},
 year = {2016}
}


@article{173ftiSzF,
 abstract = {Observational studies are rising in importance due to the widespread
accumulation of data in fields such as healthcare, education, employment and
ecology. We consider the task of answering counterfactual questions such as, "Would this patient have lower blood sugar had she received a different
medication?". We propose a new algorithmic framework for counterfactual
inference which brings together ideas from domain adaptation and representation
learning. In addition to a theoretical justification, we perform an empirical
comparison with previous approaches to causal inference from observational
data. Our deep learning algorithm significantly outperforms the previous
state-of-the-art.},
 archiveprefix = {arXiv},
 author = {Fredrik D. Johansson and Uri Shalit and David Sontag},
 eprint = {1605.03661v2},
 file = {1605.03661v2.pdf},
 month = {May},
 primaryclass = {stat.ML},
 title = {Learning Representations for Counterfactual Inference},
 url = {https://arxiv.org/abs/1605.03661v2},
 year = {2016}
}


@article{5Il3kN32,
 abstract = {Large labeled training sets are the critical building blocks of supervised
learning methods and are key enablers of deep learning techniques. For some
applications, creating labeled training sets is the most time-consuming and
expensive part of applying machine learning. We therefore propose a paradigm
for the programmatic creation of training sets called data programming in which
users express weak supervision strategies or domain heuristics as labeling
functions, which are programs that label subsets of the data, but that are
noisy and may conflict. We show that by explicitly representing this training
set labeling process as a generative model, we can "denoise" the generated
training set, and establish theoretically that we can recover the parameters of
these generative models in a handful of settings. We then show how to modify a
discriminative loss function to make it noise-aware, and demonstrate our method
over a range of discriminative models including logistic regression and LSTMs.
Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data
programming would have led to a new winning score, and also show that applying
data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points
over a state-of-the-art LSTM baseline (and into second place in the
competition). Additionally, in initial user studies we observed that data
programming may be an easier way for non-experts to create machine learning
models when training data is limited or unavailable.},
 archiveprefix = {arXiv},
 author = {Alexander Ratner and Christopher De Sa and Sen Wu and Daniel Selsam and Christopher Ré},
 eprint = {1605.07723v3},
 file = {1605.07723v3.pdf},
 month = {May},
 primaryclass = {stat.ML},
 title = {Data Programming: Creating Large Training Sets, Quickly},
 url = {https://arxiv.org/abs/1605.07723v3},
 year = {2016}
}


@article{1FE0F2pQ,
 abstract = {Previous research has shown that neural networks can model survival data in
situations in which some patients' death times are unknown, e.g.
right-censored. However, neural networks have rarely been shown to outperform
their linear counterparts such as the Cox proportional hazards model. In this
paper, we run simulated experiments and use real survival data to build upon
the risk-regression architecture proposed by Faraggi and Simon. We demonstrate
that our model, DeepSurv, not only works as well as other survival models but
actually outperforms in predictive ability on survival data with linear and
nonlinear risk functions. We then show that the neural network can also serve
as a recommender system by including a categorical variable representing a
treatment group. This can be used to provide personalized treatment
recommendations based on an individual's calculated risk. We provide an open
source Python module that implements these methods in order to advance research
on deep learning and survival analysis.},
 archiveprefix = {arXiv},
 author = {Jared Katzman and Uri Shaham and Jonathan Bates and Alexander Cloninger and Tingting Jiang and Yuval Kluger},
 eprint = {1606.00931v2},
 file = {1606.00931v2.pdf},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Deep Survival: A Deep Cox Proportional Hazards Network},
 url = {https://arxiv.org/abs/1606.00931v2},
 year = {2016}
}


@article{mbEp6jNr,
 abstract = {The International Symposium on Biomedical Imaging (ISBI) held a grand
challenge to evaluate computational systems for the automated detection of
metastatic breast cancer in whole slide images of sentinel lymph node biopsies.
Our team won both competitions in the grand challenge, obtaining an area under
the receiver operating curve (AUC) of 0.925 for the task of whole slide image
classification and a score of 0.7051 for the tumor localization task. A
pathologist independently reviewed the same images, obtaining a whole slide
image classification AUC of 0.966 and a tumor localization score of 0.733.
Combining our deep learning system's predictions with the human pathologist's
diagnoses increased the pathologist's AUC to 0.995, representing an
approximately 85 percent reduction in human error rate. These results
demonstrate the power of using deep learning to produce significant
improvements in the accuracy of pathological diagnoses.},
 archiveprefix = {arXiv},
 author = {Dayong Wang and Aditya Khosla and Rishab Gargeya and Humayun Irshad and Andrew H. Beck},
 eprint = {1606.05718v1},
 file = {1606.05718v1.pdf},
 month = {Jun},
 primaryclass = {q-bio.QM},
 title = {Deep Learning for Identifying Metastatic Breast Cancer},
 url = {https://arxiv.org/abs/1606.05718v1},
 year = {2016}
}


@article{7yE9K08a,
 abstract = {We summarize the potential impact that the European Union's new General Data
Protection Regulation will have on the routine use of machine learning
algorithms. Slated to take effect as law across the EU in 2018, it will
restrict automated individual decision-making (that is, algorithms that make
decisions based on user-level predictors) which "significantly affect" users.
The law will also effectively create a "right to explanation," whereby a user
can ask for an explanation of an algorithmic decision that was made about them.
We argue that while this law will pose large challenges for industry, it
highlights opportunities for computer scientists to take the lead in designing
algorithms and evaluation frameworks which avoid discrimination and enable
explanation.},
 archiveprefix = {arXiv},
 author = {Bryce Goodman and Seth Flaxman},
 eprint = {1606.08813v3},
 file = {1606.08813v3.pdf},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {European Union regulations on algorithmic decision-making and a "right
to explanation"},
 url = {https://arxiv.org/abs/1606.08813v3},
 year = {2016}
}


@article{ucHUOABT,
 abstract = {Machine learning techniques based on neural networks are achieving remarkable
results in a wide variety of domains. Often, the training of models requires
large, representative datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private information in these
datasets. Addressing this goal, we develop new algorithmic techniques for
learning and a refined analysis of privacy costs within the framework of
differential privacy. Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives, under a modest
privacy budget, and at a manageable cost in software complexity, training
efficiency, and model quality.},
 archiveprefix = {arXiv},
 author = {Martín Abadi and Andy Chu and Ian Goodfellow and H. Brendan McMahan and Ilya Mironov and Kunal Talwar and Li Zhang},
 doi = {10.1145/2976749.2978318},
 eprint = {1607.00133v2},
 file = {1607.00133v2.pdf},
 month = {Jul},
 primaryclass = {stat.ML},
 title = {Deep Learning with Differential Privacy},
 url = {https://arxiv.org/abs/1607.00133v2},
 year = {2016}
}


@article{Ohd1Q9Xw,
 abstract = {Feature engineering remains a major bottleneck when creating predictive
systems from electronic medical records. At present, an important missing
element is detecting predictive regular clinical motifs from irregular episodic
records. We present Deepr (short for Deep record), a new end-to-end deep
learning system that learns to extract features from medical records and
predicts future risk automatically. Deepr transforms a record into a sequence
of discrete elements separated by coded time gaps and hospital transfers. On
top of the sequence is a convolutional neural net that detects and combines
predictive local clinical motifs to stratify the risk. Deepr permits
transparent inspection and visualization of its inner working. We validate
Deepr on hospital data to predict unplanned readmission after discharge. Deepr
achieves superior accuracy compared to traditional techniques, detects
meaningful clinical motifs, and uncovers the underlying structure of the
disease and intervention space.},
 archiveprefix = {arXiv},
 author = {Phuoc Nguyen and Truyen Tran and Nilmini Wickramasinghe and Svetha Venkatesh},
 eprint = {1607.07519v1},
 file = {1607.07519v1.pdf},
 month = {Jul},
 primaryclass = {stat.ML},
 title = {Deepr: A Convolutional Net for Medical Records},
 url = {https://arxiv.org/abs/1607.07519v1},
 year = {2016}
}


@article{c6MfDdWP,
 abstract = {Disparate areas of machine learning have benefited from models that can take
raw data with little preprocessing as input and learn rich representations of
that raw data in order to perform well on a given prediction task. We evaluate
this approach in healthcare by using longitudinal measurements of lab tests, one of the more raw signals of a patient's health state widely available in
clinical data, to predict disease onsets. In particular, we train a Long
Short-Term Memory (LSTM) recurrent neural network and two novel convolutional
neural networks for multi-task prediction of disease onset for 133 conditions
based on 18 common lab tests measured over time in a cohort of 298K patients
derived from 8 years of administrative claims data. We compare the neural
networks to a logistic regression with several hand-engineered, clinically
relevant features. We find that the representation-based learning approaches
significantly outperform this baseline. We believe that our work suggests a new
avenue for patient risk stratification based solely on lab results.},
 archiveprefix = {arXiv},
 author = {Narges Razavian and Jake Marcus and David Sontag},
 eprint = {1608.00647v3},
 file = {1608.00647v3.pdf},
 month = {Aug},
 primaryclass = {cs.LG},
 title = {Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests},
 url = {https://arxiv.org/abs/1608.00647v3},
 year = {2016}
}


@article{qXdO2aMm,
 abstract = {The electronic health record (EHR) provides an unprecedented opportunity to
build actionable tools to support physicians at the point of care. In this
paper, we investigate survival analysis in the context of EHR data. We
introduce deep survival analysis, a hierarchical generative approach to
survival analysis. It departs from previous approaches in two primary ways: (1)
all observations, including covariates, are modeled jointly conditioned on a
rich latent structure; and (2) the observations are aligned by their failure
time, rather than by an arbitrary time zero as in traditional survival
analysis. Further, it (3) scalably handles heterogeneous (continuous and
discrete) data types that occur in the EHR. We validate deep survival analysis
model by stratifying patients according to risk of developing coronary heart
disease (CHD). Specifically, we study a dataset of 313,000 patients
corresponding to 5.5 million months of observations. When compared to the
clinically validated Framingham CHD risk score, deep survival analysis is
significantly superior in stratifying patients according to their risk.},
 archiveprefix = {arXiv},
 author = {Rajesh Ranganath and Adler Perotte and Noémie Elhadad and David Blei},
 eprint = {1608.02158v2},
 file = {1608.02158v2.pdf},
 month = {Aug},
 primaryclass = {stat.ML},
 title = {Deep Survival Analysis},
 url = {https://arxiv.org/abs/1608.02158v2},
 year = {2016}
}


@article{ULSPV0rh,
 abstract = {Machine learning (ML) models may be deemed confidential due to their
sensitive training data, commercial value, or use in security applications.
Increasingly often, confidential ML models are being deployed with publicly
accessible query interfaces. ML-as-a-service ("predictive analytics") systems
are an example: Some allow users to train models on potentially sensitive data
and charge others for access on a pay-per-query basis.
The tension between model confidentiality and public access motivates our
investigation of model extraction attacks. In such attacks, an adversary with
black-box access, but no prior knowledge of an ML model's parameters or
training data, aims to duplicate the functionality of (i.e., "steal") the
model. Unlike in classical learning theory settings, ML-as-a-service offerings
may accept partial feature vectors as inputs and include confidence values with
predictions. Given these practices, we show simple, efficient attacks that
extract target ML models with near-perfect fidelity for popular model classes
including logistic regression, neural networks, and decision trees. We
demonstrate these attacks against the online services of BigML and Amazon
Machine Learning. We further show that the natural countermeasure of omitting
confidence values from model outputs still admits potentially harmful model
extraction attacks. Our results highlight the need for careful ML model
deployment and new model extraction countermeasures.},
 archiveprefix = {arXiv},
 author = {Florian Tramèr and Fan Zhang and Ari Juels and Michael K. Reiter and Thomas Ristenpart},
 eprint = {1609.02943v2},
 file = {1609.02943v2.pdf},
 month = {Sep},
 primaryclass = {cs.CR},
 title = {Stealing Machine Learning Models via Prediction APIs},
 url = {https://arxiv.org/abs/1609.02943v2},
 year = {2016}
}


@article{4TK06zOf,
 abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for
automated translation, with the potential to overcome many of the weaknesses of
conventional phrase-based translation systems. Unfortunately, NMT systems are
known to be computationally expensive both in training and in translation
inference. Also, most NMT systems have difficulty with rare words. These issues
have hindered NMT's use in practical deployments and services, where both
accuracy and speed are essential. In this work, we present GNMT, Google's
Neural Machine Translation system, which attempts to address many of these
issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder
layers using attention and residual connections. To improve parallelism and
therefore decrease training time, our attention mechanism connects the bottom
layer of the decoder to the top layer of the encoder. To accelerate the final
translation speed, we employ low-precision arithmetic during inference
computations. To improve handling of rare words, we divide words into a limited
set of common sub-word units ("wordpieces") for both input and output. This
method provides a good balance between the flexibility of "character"-delimited
models and the efficiency of "word"-delimited models, naturally handles
translation of rare words, and ultimately improves the overall accuracy of the
system. Our beam search technique employs a length-normalization procedure and
uses a coverage penalty, which encourages generation of an output sentence that
is most likely to cover all the words in the source sentence. On the WMT'14
English-to-French and English-to-German benchmarks, GNMT achieves competitive
results to state-of-the-art. Using a human side-by-side evaluation on a set of
isolated simple sentences, it reduces translation errors by an average of 60%
compared to Google's phrase-based production system.},
 archiveprefix = {arXiv},
 author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
 eprint = {1609.08144v2},
 file = {1609.08144v2.pdf},
 month = {Sep},
 primaryclass = {cs.CL},
 title = {Google's Neural Machine Translation System: Bridging the Gap between
Human and Machine Translation},
 url = {https://arxiv.org/abs/1609.08144v2},
 year = {2016}
}


@article{1ENxzq6pT,
 abstract = {We propose a criterion for discrimination against a specified sensitive
attribute in supervised learning, where the goal is to predict some target
based on available features. Assuming data about the predictor, target, and
membership in the protected group are available, we show how to optimally
adjust any learned predictor so as to remove discrimination according to our
definition. Our framework also improves incentives by shifting the cost of poor
classification from disadvantaged groups to the decision maker, who can respond
by improving the classification accuracy.
In line with other studies, our notion is oblivious: it depends only on the
joint statistics of the predictor, the target and the protected attribute, but
not on interpretation of individualfeatures. We study the inherent limits of
defining and identifying biases based on such oblivious measures, outlining
what can and cannot be inferred from different oblivious tests.
We illustrate our notion using a case study of FICO credit scores.},
 archiveprefix = {arXiv},
 author = {Moritz Hardt and Eric Price and Nathan Srebro},
 eprint = {1610.02413v1},
 file = {1610.02413v1.pdf},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://arxiv.org/abs/1610.02413v1},
 year = {2016}
}


@article{M2OLWojE,
 abstract = {Conversational speech recognition has served as a flagship speech recognition
task since the release of the Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find
that our latest automated system has reached human parity. The error rate of
professional transcribers is 5.9% for the Switchboard portion of the data, in
which newly acquainted pairs of people discuss an assigned topic, and 11.3% for
the CallHome portion where friends and family members have open-ended
conversations. In both cases, our automated system establishes a new state of
the art, and edges past the human benchmark, achieving error rates of 5.8% and
11.0%, respectively. The key to our system's performance is the use of various
convolutional and LSTM acoustic model architectures, combined with a novel
spatial smoothing method and lattice-free MMI acoustic training, multiple
recurrent neural network language modeling approaches, and a systematic use of
system combination.},
 archiveprefix = {arXiv},
 author = {W. Xiong and J. Droppo and X. Huang and F. Seide and M. Seltzer and A. Stolcke and D. Yu and G. Zweig},
 eprint = {1610.05256v2},
 file = {1610.05256v2.pdf},
 month = {Nov},
 primaryclass = {cs.CL},
 title = {Achieving Human Parity in Conversational Speech Recognition},
 url = {https://arxiv.org/abs/1610.05256v2},
 year = {2016}
}


@article{1HbRTExaU,
 abstract = {We quantitatively investigate how machine learning models leak information
about the individual data records on which they were trained. We focus on the
basic membership inference attack: given a data record and black-box access to
a model, determine if the record was in the model's training dataset. To
perform membership inference against a target model, we make adversarial use of
machine learning and train our own inference model to recognize differences in
the target model's predictions on the inputs that it trained on versus the
inputs that it did not train on.
We empirically evaluate our inference techniques on classification models
trained by commercial "machine learning as a service" providers such as Google
and Amazon. Using realistic datasets and classification tasks, including a
hospital discharge dataset whose membership is sensitive from the privacy
perspective, we show that these models can be vulnerable to membership
inference attacks. We then investigate the factors that influence this leakage
and evaluate mitigation strategies.},
 archiveprefix = {arXiv},
 author = {Reza Shokri and Marco Stronati and Congzheng Song and Vitaly Shmatikov},
 eprint = {1610.05820v2},
 file = {1610.05820v2.pdf},
 month = {Nov},
 primaryclass = {cs.CR},
 title = {Membership Inference Attacks against Machine Learning Models},
 url = {https://arxiv.org/abs/1610.05820v2},
 year = {2016}
}


@article{11aqfNfQx,
 abstract = {We study fairness in linear bandit problems. Starting from the notion of
meritocratic fairness introduced in Joseph et al. [2016], we carry out a more
refined analysis of a more general problem, achieving better performance
guarantees with fewer modelling assumptions on the number and structure of
available choices as well as the number selected. We also analyze the
previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds
demonstrating that this instance-dependence is necessary. The result is a
framework for meritocratic fairness in an online linear setting that is
substantially more powerful, general, and realistic than the current state of
the art.},
 archiveprefix = {arXiv},
 author = {Matthew Joseph and Michael Kearns and Jamie Morgenstern and Seth Neel and Aaron Roth},
 eprint = {1610.09559v4},
 file = {1610.09559v4.pdf},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Fair Algorithms for Infinite and Contextual Bandits},
 url = {https://arxiv.org/abs/1610.09559v4},
 year = {2016}
}


@article{lERqKdZJ,
 abstract = {This paper proposes a general method for improving the structure and quality
of sequences generated by a recurrent neural network (RNN), while maintaining
information originally learned from data, as well as sample diversity. An RNN
is first pre-trained on data using maximum likelihood estimation (MLE), and the
probability distribution over the next token in the sequence learned by this
model is treated as a prior policy. Another RNN is then trained using
reinforcement learning (RL) to generate higher-quality outputs that account for
domain-specific incentives while retaining proximity to the prior policy of the
MLE RNN. To formalize this objective, we derive novel off-policy RL methods for
RNNs from KL-control. The effectiveness of the approach is demonstrated on two
applications; 1) generating novel musical melodies, and 2) computational
molecular generation. For both problems, we show that the proposed method
improves the desired properties and structure of the generated sequences, while
maintaining information learned from data.},
 archiveprefix = {arXiv},
 author = {Natasha Jaques and Shixiang Gu and Dzmitry Bahdanau and José Miguel Hernández-Lobato and Richard E. Turner and Douglas Eck},
 eprint = {1611.02796v8},
 file = {1611.02796v8.pdf},
 month = {Dec},
 primaryclass = {cs.LG},
 title = {Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models
with KL-control},
 url = {https://arxiv.org/abs/1611.02796v8},
 year = {2016}
}


@article{AsLAb71x,
 abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying
array of applications such as data analytics, autonomous systems, and security
diagnostics. ML is now pervasive---new systems and models are being deployed in
every domain imaginable, leading to rapid and widespread deployment of software
based inference and decision making. There is growing recognition that ML
exposes new vulnerabilities in software systems, yet the technical community's
understanding of the nature and extent of these vulnerabilities remains
limited. We systematize recent findings on ML security and privacy, focusing on
attacks identified on these systems and defenses crafted to date. We articulate
a comprehensive threat model for ML, and categorize attacks and defenses within
an adversarial framework. Key insights resulting from works both in the ML and
security communities are identified and the effectiveness of approaches are
related to structural elements of ML algorithms and the data used to train
them. We conclude by formally exploring the opposing relationship between model
accuracy and resilience to adversarial manipulation. Through these
explorations, we show that there are (possibly unavoidable) tensions between
model complexity, accuracy, and resilience that must be calibrated for the
environments in which they will be used.},
 archiveprefix = {arXiv},
 author = {Nicolas Papernot and Patrick McDaniel and Arunesh Sinha and Michael Wellman},
 eprint = {1611.03814v1},
 file = {1611.03814v1.pdf},
 month = {Dec},
 primaryclass = {cs.CR},
 title = {Towards the Science of Security and Privacy in Machine Learning},
 url = {https://arxiv.org/abs/1611.03814v1},
 year = {2016}
}


@article{dO844vZn,
 abstract = {Automated extraction of concepts from patient clinical records is an
essential facilitator of clinical research. For this reason, the 2010 i2b2/VA
Natural Language Processing Challenges for Clinical Records introduced a
concept extraction task aimed at identifying and classifying concepts into
predefined categories (i.e., treatments, tests and problems). State-of-the-art
concept extraction approaches heavily rely on handcrafted features and
domain-specific resources which are hard to collect and define. For this
reason, this paper proposes an alternative, streamlined approach: a recurrent
neural network (the bidirectional LSTM with CRF decoding) initialized with
general-purpose, off-the-shelf word embeddings. The experimental results
achieved on the 2010 i2b2/VA reference corpora using the proposed framework
outperform all recent methods and ranks closely to the best submission from the
original 2010 i2b2/VA challenge.},
 archiveprefix = {arXiv},
 author = {Raghavendra Chalapathy and Ehsan Zare Borzeshi and Massimo Piccardi},
 eprint = {1611.08373v1},
 file = {1611.08373v1.pdf},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Bidirectional LSTM-CRF for Clinical Concept Extraction},
 url = {https://arxiv.org/abs/1611.08373v1},
 year = {2016}
}


@article{apBChoyF,
 abstract = {The recent rapid and tremendous success of deep convolutional neural networks
(CNN) on many challenging computer vision tasks largely derives from the
accessibility of the well-annotated ImageNet and PASCAL VOC datasets.
Nevertheless, unsupervised image categorization (i.e., without the ground-truth
labeling) is much less investigated, yet critically important and difficult
when annotations are extremely hard to obtain in the conventional way of
"Google Search" and crowd sourcing. We address this problem by presenting a
looped deep pseudo-task optimization (LDPO) framework for joint mining of deep
CNN features and image labels. Our method is conceptually simple and rests upon
the hypothesized "convergence" of better labels leading to better trained CNN
models which in turn feed more discriminative image representations to
facilitate more meaningful clusters/labels. Our proposed method is validated in
tackling two important applications: 1) Large-scale medical image annotation
has always been a prohibitively expensive and easily-biased task even for
well-trained radiologists. Significantly better image categorization results
are achieved via our proposed approach compared to the previous
state-of-the-art method. 2) Unsupervised scene recognition on representative
and publicly available datasets with our proposed technique is examined. The
LDPO achieves excellent quantitative scene classification results. On the MIT
indoor scene dataset, it attains a clustering accuracy of 75.3%, compared to
the state-of-the-art supervised classification accuracy of 81.0% (when both are
based on the VGG-VD model).},
 archiveprefix = {arXiv},
 author = {Xiaosong Wang and Le Lu and Hoo-chang Shin and Lauren Kim and Mohammadhadi Bagheri and Isabella Nogues and Jianhua Yao and Ronald M. Summers},
 eprint = {1701.06599v1},
 file = {1701.06599v1.pdf},
 month = {Jan},
 primaryclass = {cs.CV},
 title = {Unsupervised Joint Mining of Deep Features and Image Labels for
Large-scale Radiology Image Categorization and Scene Recognition},
 url = {https://arxiv.org/abs/1701.06599v1},
 year = {2017}
}


@article{AQ3N6Ayw,
 abstract = {Deep generative models have been wildly successful at learning coherent
latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and
molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the
key observation that frequently, discrete data can be represented as a parse
tree from a context-free grammar. We propose a variational autoencoder which
encodes and decodes directly to and from these parse trees, ensuring the
generated outputs are always valid. Surprisingly, we show that not only does
our model more often generate valid outputs, it also learns a more coherent
latent space in which nearby points decode to similar discrete outputs. We
demonstrate the effectiveness of our learned models by showing their improved
performance in Bayesian optimization for symbolic regression and molecular
synthesis.},
 archiveprefix = {arXiv},
 author = {Matt J. Kusner and Brooks Paige and José Miguel Hernández-Lobato},
 eprint = {1703.01925v1},
 file = {1703.01925v1.pdf},
 month = {Mar},
 primaryclass = {stat.ML},
 title = {Grammar Variational Autoencoder},
 url = {https://arxiv.org/abs/1703.01925v1},
 year = {2017}
}


@article{wKioubsT,
 abstract = {One of the most difficult speech recognition tasks is accurate recognition of
human to human communication. Advances in deep learning over the last few years
have produced major speech recognition improvements on the representative
Switchboard conversational corpus. Word error rates that just a few years ago
were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now
believed to be within striking range of human performance. This then raises two
issues - what IS human performance, and how far down can we still drive speech
recognition error rates? A recent paper by Microsoft suggests that we have
already achieved human performance. In trying to verify this statement, we
performed an independent set of human performance measurements on two
conversational tasks and found that human performance may be considerably
better than what was earlier reported, giving the community a significantly
harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the
word error rate of our own English conversational telephone LVCSR system to the
level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000
evaluation, which - at least at the writing of this paper - is a new
performance milestone (albeit not at what we measure to be human performance!).
On the acoustic side, we use a score fusion of three models: one LSTM with
multiple feature inputs, a second LSTM trained with speaker-adversarial
multi-task learning and a third residual net (ResNet) with 25 convolutional
layers and time-dilated convolutions. On the language modeling side, we use
word and character LSTMs and convolutional WaveNet-style language models.},
 archiveprefix = {arXiv},
 author = {George Saon and Gakuto Kurata and Tom Sercu and Kartik Audhkhasi and Samuel Thomas and Dimitrios Dimitriadis and Xiaodong Cui and Bhuvana Ramabhadran and Michael Picheny and Lynn-Li Lim and Bergul Roomi and Phil Hall},
 eprint = {1703.02136v1},
 file = {1703.02136v1.pdf},
 month = {Mar},
 primaryclass = {cs.CL},
 title = {English Conversational Telephone Speech Recognition by Humans and
Machines},
 url = {https://arxiv.org/abs/1703.02136v1},
 year = {2017}
}


@article{xl1ijigK,
 abstract = {Access to electronic health records (EHR) data has motivated computational
advances in medical research. However, various concerns, particularly over
privacy, can limit access to and collaborative use of EHR data. Sharing
synthetic EHR data could mitigate risk. In this paper, we propose a new
approach, medical Generative Adversarial Network (medGAN), to generate
realistic synthetic EHRs. Based on an input EHR dataset, medGAN can generate
high-dimensional discrete variables (e.g., binary and count features) via a
combination of an autoencoder and generative adversarial networks. We also
propose minibatch averaging to efficiently avoid mode collapse, and increase
the learning efficiency with batch normalization and shortcut connections. To
demonstrate feasibility, we showed that medGAN generates synthetic EHR datasets
that achieve comparable performance to real data on many experiments including
distribution statistics, predictive modeling tasks and medical expert review.},
 archiveprefix = {arXiv},
 author = {Edward Choi and Siddharth Biswal and Bradley Malin and Jon Duke and Walter F. Stewart and Jimeng Sun},
 eprint = {1703.06490v1},
 file = {1703.06490v1.pdf},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {Generating Multi-label Discrete Electronic Health Records using
Generative Adversarial Networks},
 url = {https://arxiv.org/abs/1703.06490v1},
 year = {2017}
}


@article{17YaKNLKk,
 abstract = {Empirical scoring functions based on either molecular force fields or
cheminformatics descriptors are widely used, in conjunction with molecular
docking, during the early stages of drug discovery to predict potency and
binding affinity of a drug-like molecule to a given target. These models
require expert-level knowledge of physical chemistry and biology to be encoded
as hand-tuned parameters or features rather than allowing the underlying model
to select features in a data-driven procedure. Here, we develop a general
3-dimensional spatial convolution operation for learning atomic-level chemical
interactions directly from atomic coordinates and demonstrate its application
to structure-based bioactivity prediction. The atomic convolutional neural
network is trained to predict the experimentally determined binding affinity of
a protein-ligand complex by direct calculation of the energy associated with
the complex, protein, and ligand given the crystal structure of the binding
pose. Non-covalent interactions present in the complex that are absent in the
protein-ligand sub-structures are identified and the model learns the
interaction strength associated with these features. We test our model by
predicting the binding free energy of a subset of protein-ligand complexes
found in the PDBBind dataset and compare with state-of-the-art cheminformatics
and machine learning-based approaches. We find that all methods achieve
experimental accuracy and that atomic convolutional networks either outperform
or perform competitively with the cheminformatics based methods. Unlike all
previous protein-ligand prediction systems, atomic convolutional networks are
end-to-end and fully-differentiable. They represent a new data-driven, physics-based deep learning model paradigm that offers a strong foundation for
future improvements in structure-based bioactivity prediction.},
 archiveprefix = {arXiv},
 author = {Joseph Gomes and Bharath Ramsundar and Evan N. Feinberg and Vijay S. Pande},
 eprint = {1703.10603v1},
 file = {1703.10603v1.pdf},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {Atomic Convolutional Networks for Predicting Protein-Ligand Binding
Affinity},
 url = {https://arxiv.org/abs/1703.10603v1},
 year = {2017}
}


@article{18lZK7fxH,
 abstract = {Although deep neural networks (DNNs) have achieved great success in many
computer vision tasks, recent studies have shown they are vulnerable to
adversarial examples. Such examples, typically generated by adding small but
purposeful distortions, can frequently fool DNN models. Previous studies to
defend against adversarial examples mostly focused on refining the DNN models.
They have either shown limited success or suffer from the expensive
computation. We propose a new strategy, \emph{feature squeezing}, that can be
used to harden DNN models by detecting adversarial examples. Feature squeezing
reduces the search space available to an adversary by coalescing samples that
correspond to many different feature vectors in the original space into a
single sample. By comparing a DNN model's prediction on the original input with
that on the squeezed input, feature squeezing detects adversarial examples with
high accuracy and few false positives. This paper explores two instances of
feature squeezing: reducing the color bit depth of each pixel and smoothing
using a spatial filter. These strategies are straightforward, inexpensive, and
complementary to defensive methods that operate on the underlying model, such
as adversarial training.},
 archiveprefix = {arXiv},
 author = {Weilin Xu and David Evans and Yanjun Qi},
 eprint = {1704.01155v1},
 file = {1704.01155v1.pdf},
 month = {Apr},
 primaryclass = {cs.CV},
 title = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural
Networks},
 url = {https://arxiv.org/abs/1704.01155v1},
 year = {2017}
}


@article{ULagTifF,
 abstract = {Many architects believe that major improvements in cost-energy-performance
must now come from domain-specific hardware. This paper evaluates a custom
ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since
2015 that accelerates the inference phase of neural networks (NN). The heart of
the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak
throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed
on-chip memory. The TPU's deterministic execution model is a better match to
the 99th-percentile response-time requirement of our NN applications than are
the time-varying optimizations of CPUs and GPUs (caches, out-of-order
execution, multithreading, multiprocessing, prefetching, ...) that help average
throughput more than guaranteed latency. The lack of such features helps
explain why, despite having myriad MACs and a big memory, the TPU is relatively
small and low power. We compare the TPU to a server-class Intel Haswell CPU and
an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters.
Our workload, written in the high-level TensorFlow framework, uses production
NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters'
NN inference demand. Despite low utilization for some applications, the TPU is
on average about 15X - 30X faster than its contemporary GPU or CPU, with
TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the
TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and
200X the CPU.},
 archiveprefix = {arXiv},
 author = {Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre-luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross and Matt Ross and Amir Salek and Emad Samadiani and Chris Severn and Gregory Sizikov and Matthew Snelham and Jed Souter and Dan Steinberg and Andy Swing and Mercedes Tan and Gregory Thorson and Bo Tian and Horia Toma and Erick Tuttle and Vijay Vasudevan and Richard Walter and Walter Wang and Eric Wilcox and Doe Hyun Yoon},
 eprint = {1704.04760v1},
 file = {1704.04760v1.pdf},
 month = {Apr},
 primaryclass = {cs.AR},
 title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
 url = {https://arxiv.org/abs/1704.04760v1},
 year = {2017}
}


@article{39RPiE10,
 abstract = {Computational prediction of membrane protein (MP) structures is very
challenging partially due to lack of sufficient solved structures for homology
modeling. Recently direct evolutionary coupling analysis (DCA) sheds some light
on protein contact prediction and accordingly, contact-assisted folding, but
DCA is effective only on some very large-sized families since it uses
information only in a single protein family. This paper presents a deep
transfer learning method that can significantly improve MP contact prediction
by learning contact patterns and complex sequence-contact relationship from
thousands of non-membrane proteins (non-MPs). Tested on 510 non-redundant MPs, our deep model (learned from only non-MPs) has top L/10 long-range contact
prediction accuracy 0.69, better than our deep model trained by only MPs (0.63)
and much better than a representative DCA method CCMpred (0.47) and the CASP11
winner MetaPSICOV (0.55). The accuracy of our deep model can be further
improved to 0.72 when trained by a mix of non-MPs and MPs. When only contacts
in transmembrane regions are evaluated, our method has top L/10 long-range
accuracy 0.62, 0.57, and 0.53 when trained by a mix of non-MPs and MPs, by
non-MPs only, and by MPs only, respectively, still much better than MetaPSICOV
(0.45) and CCMpred (0.40). All these results suggest that sequence-structure
relationship learned by our deep model from non-MPs generalizes well to MP
contact prediction. Improved contact prediction also leads to better
contact-assisted folding. Using only top predicted contacts as restraints, our
deep learning method can fold 160 and 200 of 510 MPs with TMscore>0.6 when
trained by non-MPs only and by a mix of non-MPs and MPs, respectively, while
CCMpred and MetaPSICOV can do so for only 56 and 77 MPs, respectively. Our
contact-assisted folding also greatly outperforms homology modeling.},
 archiveprefix = {arXiv},
 author = {Zhen Li and Sheng Wang and Yizhou Yu and Jinbo Xu},
 eprint = {1704.07207v1},
 file = {1704.07207v1.pdf},
 month = {Apr},
 primaryclass = {q-bio.BM},
 title = {Predicting membrane protein contacts from non-membrane proteins by deep
transfer learning},
 url = {https://arxiv.org/abs/1704.07207v1},
 year = {2017}
}


@article{PGi9g7yV,
 abstract = {The chest X-ray is one of the most commonly accessible radiological
examinations for screening and diagnosis of many lung diseases. A tremendous
number of X-ray imaging studies accompanied by radiological reports are
accumulated and stored in many modern hospitals' Picture Archiving and
Communication Systems (PACS). On the other side, it is still an open question
how this type of hospital-size knowledge database containing invaluable imaging
informatics (i.e., loosely labeled) can be used to facilitate the data-hungry
deep learning paradigms in building truly large-scale high precision
computer-aided diagnosis (CAD) systems.
In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients
with the text-mined eight disease image labels (where each image can have
multi-labels), from the associated radiological reports using natural language
processing. Importantly, we demonstrate that these commonly occurring thoracic
diseases can be detected and even spatially-located via a unified
weakly-supervised multi-label image classification and disease localization
framework, which is validated using our proposed dataset. Although the initial
quantitative results are promising as reported, deep convolutional neural
network based "reading chest X-rays" (i.e., recognizing and locating the common
disease patterns trained with only image-level labels) remains a strenuous task
for fully-automated high precision CAD systems.},
 archiveprefix = {arXiv},
 author = {Xiaosong Wang and Yifan Peng and Le Lu and Zhiyong Lu and Mohammadhadi Bagheri and Ronald M. Summers},
 eprint = {1705.02315v3},
 file = {1705.02315v3.pdf},
 month = {May},
 primaryclass = {cs.CV},
 title = {ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on
Weakly-Supervised Classification and Localization of Common Thorax Diseases},
 url = {https://arxiv.org/abs/1705.02315v3},
 year = {2017}
}


@article{haHzVaaz,
 abstract = {Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.},
 archiveprefix = {arXiv},
 author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
 eprint = {1409.0473v7},
 file = {1409.0473v7.pdf},
 month = {Sep},
 primaryclass = {cs.CL},
 title = {Neural Machine Translation by Jointly Learning to Align and Translate},
 url = {https://arxiv.org/abs/1409.0473v7},
 year = {2014}
}


@article{1G3owNNps,
 abstract = {Multipliers are the most space and power-hungry arithmetic operators of the
digital implementation of deep neural networks. We train a set of
state-of-the-art neural networks (Maxout networks) on three benchmark datasets:
MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats:
floating point, fixed point and dynamic fixed point. For each of those datasets
and for each of those formats, we assess the impact of the precision of the
multiplications on the final error after training. We find that very low
precision is sufficient not just for running trained networks but also for
training them. For example, it is possible to train Maxout networks with 10
bits multiplications.},
 archiveprefix = {arXiv},
 author = {Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
 eprint = {1412.7024v5},
 file = {1412.7024v5.pdf},
 month = {12},
 primaryclass = {cs.LG},
 title = {Training deep neural networks with low precision multiplications},
 url = {https://arxiv.org/abs/1412.7024v5},
 year = {2014}
}


@article{1BTJ1KqRa,
 abstract = {Motivation: The MinION device by Oxford Nanopore is the first portable
sequencing device. MinION is able to produce very long reads (reads over
100~kBp were reported), however it suffers from high sequencing error rate. In
this paper, we show that the error rate can be reduced by improving the base
calling process.
Results: We present the first open-source DNA base caller for the MinION
sequencing platform by Oxford Nanopore. By employing carefully crafted
recurrent neural networks, our tool improves the base calling accuracy compared
to the default base caller supplied by the manufacturer. This advance may
further enhance applicability of MinION for genome sequencing and various
clinical applications.
Availability: DeepNano can be downloaded at
http://compbio.fmph.uniba.sk/deepnano/.
Contact: boza@fmph.uniba.sk},
 archiveprefix = {arXiv},
 author = {Vladimír Boža and Broňa Brejová and Tomáš Vinař},
 doi = {10.1371/journal.pone.0178751},
 eprint = {1603.09195v1},
 file = {1603.09195v1.pdf},
 month = {Mar},
 note = {PLoS ONE 12(6): e0178751},
 primaryclass = {q-bio.GN},
 title = {DeepNano: Deep Recurrent Neural Networks for Base Calling in MinION
Nanopore Reads},
 url = {https://arxiv.org/abs/1603.09195v1},
 year = {2016}
}


@article{1AhGoHZP9,
 abstract = {Currently, deep neural networks are the state of the art on problems such as
speech recognition and computer vision. In this extended abstract, we show that
shallow feed-forward networks can learn the complex functions previously
learned by deep nets and achieve accuracies previously only achievable with
deep models. Moreover, in some cases the shallow neural nets can learn these
deep functions using a total number of parameters similar to the original deep
model. We evaluate our method on the TIMIT phoneme recognition task and are
able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training
shallow neural nets to mimic deeper models suggests that there probably exist
better algorithms for training shallow feed-forward nets than those currently
available.},
 archiveprefix = {arXiv},
 author = {Lei Jimmy Ba and Rich Caruana},
 eprint = {1312.6184v7},
 file = {1312.6184v7.pdf},
 month = {12},
 primaryclass = {cs.LG},
 title = {Do Deep Nets Really Need to be Deep?},
 url = {https://arxiv.org/abs/1312.6184v7},
 year = {2013}
}


@article{14DAmZTDg,
 abstract = {Exponential growth in Electronic Healthcare Records (EHR) has resulted in new
opportunities and urgent needs for discovery of meaningful data-driven
representations and patterns of diseases in Computational Phenotyping research.
Deep Learning models have shown superior performance for robust prediction in
computational phenotyping tasks, but suffer from the issue of model
interpretability which is crucial for clinicians involved in decision-making.
In this paper, we introduce a novel knowledge-distillation approach called
Interpretable Mimic Learning, to learn interpretable phenotype features for
making robust prediction while mimicking the performance of deep learning
models. Our framework uses Gradient Boosting Trees to learn interpretable
features from deep learning models such as Stacked Denoising Autoencoder and
Long Short-Term Memory. Exhaustive experiments on a real-world clinical
time-series dataset show that our method obtains similar or better performance
than the deep learning models, and it provides interpretable phenotypes for
clinical decision making.},
 archiveprefix = {arXiv},
 author = {Zhengping Che and Sanjay Purushotham and Robinder Khemani and Yan Liu},
 eprint = {1512.03542v1},
 file = {1512.03542v1.pdf},
 month = {12},
 primaryclass = {stat.ML},
 title = {Distilling Knowledge from Deep Networks with Applications to Healthcare
Domain},
 url = {https://arxiv.org/abs/1512.03542v1},
 year = {2015}
}


@article{O7Vbecm2,
 abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In
time series prediction and other related tasks, it has been noted that missing
values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the
missing patterns for effective imputation and improving prediction performance.
In this paper, we develop novel deep learning models, namely GRU-D, as one of
the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a
state-of-the-art recurrent neural network. It takes two representations of
missing patterns, i.e., masking and time interval, and effectively incorporates
them into a deep model architecture so that it not only captures the long-term
temporal dependencies in time series, but also utilizes the missing patterns to
achieve better prediction results. Experiments of time series classification
tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic
datasets demonstrate that our models achieve state-of-the-art performance and
provides useful insights for better understanding and utilization of missing
values in time series analysis.},
 archiveprefix = {arXiv},
 author = {Zhengping Che and Sanjay Purushotham and Kyunghyun Cho and David Sontag and Yan Liu},
 eprint = {1606.01865v2},
 file = {1606.01865v2.pdf},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Recurrent Neural Networks for Multivariate Time Series with Missing
Values},
 url = {https://arxiv.org/abs/1606.01865v2},
 year = {2016}
}


@article{15lYGmZpY,
 abstract = {As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow
models to absorb ever-increasing data set sizes; however mobile devices are
designed with very little memory and cannot store such large models. We present
a novel network architecture, HashedNets, that exploits inherent redundancy in
neural networks to achieve drastic reductions in model sizes. HashedNets uses a
low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value.
These parameters are tuned to adjust to the HashedNets weight sharing
architecture with standard backprop during training. Our hashing procedure
introduces no additional memory overhead, and we demonstrate on several
benchmark data sets that HashedNets shrink the storage requirements of neural
networks substantially while mostly preserving generalization performance.},
 archiveprefix = {arXiv},
 author = {Wenlin Chen and James T. Wilson and Stephen Tyree and Kilian Q. Weinberger and Yixin Chen},
 eprint = {1504.04788v1},
 file = {1504.04788v1.pdf},
 month = {Apr},
 primaryclass = {cs.LG},
 title = {Compressing Neural Networks with the Hashing Trick},
 url = {https://arxiv.org/abs/1504.04788v1},
 year = {2015}
}


@article{10nDTiETi,
 abstract = {Deep learning methods exhibit promising performance for predictive modeling
in healthcare, but two important challenges remain: -Data insufficiency:Often
in healthcare predictive modeling, the sample size is insufficient for deep
learning methods to achieve satisfactory results. -Interpretation:The
representations learned by deep learning methods should align with medical
knowledge. To address these challenges, we propose a GRaph-based Attention
Model, GRAM that supplements electronic health records (EHR) with hierarchical
information inherent to medical ontologies. Based on the data volume and the
ontology structure, GRAM represents a medical concept as a combination of its
ancestors in the ontology via an attention mechanism. We compared predictive
performance (i.e. accuracy, data needs, interpretability) of GRAM to various
methods including the recurrent neural network (RNN) in two sequential
diagnoses prediction tasks and one heart failure prediction task. Compared to
the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely
observed in the training data and 3% improved area under the ROC curve for
predicting heart failure using an order of magnitude less training data.
Additionally, unlike other methods, the medical concept representations learned
by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits
intuitive attention behaviors by adaptively generalizing to higher level
concepts when facing data insufficiency at the lower level concepts.},
 archiveprefix = {arXiv},
 author = {Edward Choi and Mohammad Taha Bahadori and Le Song and Walter F. Stewart and Jimeng Sun},
 eprint = {1611.07012v3},
 file = {1611.07012v3.pdf},
 month = {Dec},
 primaryclass = {cs.LG},
 title = {GRAM: Graph-based Attention Model for Healthcare Representation Learning},
 url = {https://arxiv.org/abs/1611.07012v3},
 year = {2016}
}


@article{UcRbawKo,
 abstract = {Accuracy and interpretability are two dominant features of successful
predictive models. Typically, a choice must be made in favor of complex black
box models such as recurrent neural networks (RNN) for accuracy versus less
accurate but more interpretable traditional models such as logistic regression.
This tradeoff poses challenges in medicine where both accuracy and
interpretability are important. We addressed this challenge by developing the
REverse Time AttentIoN model (RETAIN) for application to Electronic Health
Records (EHR) data. RETAIN achieves high accuracy while remaining clinically
interpretable and is based on a two-level neural attention model that detects
influential past visits and significant clinical variables within those visits
(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR
data in a reverse time order so that recent clinical visits are likely to
receive higher attention. RETAIN was tested on a large health system EHR
dataset with 14 million visits completed by 263K patients over an 8 year period
and demonstrated predictive accuracy and computational scalability comparable
to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.},
 archiveprefix = {arXiv},
 author = {Edward Choi and Mohammad Taha Bahadori and Joshua A. Kulas and Andy Schuetz and Walter F. Stewart and Jimeng Sun},
 eprint = {1608.05745v4},
 file = {1608.05745v4.pdf},
 month = {Aug},
 primaryclass = {cs.LG},
 title = {RETAIN: An Interpretable Predictive Model for Healthcare using Reverse
Time Attention Mechanism},
 url = {https://arxiv.org/abs/1608.05745v4},
 year = {2016}
}


@article{VMkPJjVk,
 abstract = {We present an interpretation of Inception modules in convolutional neural
networks as being an intermediate step in-between regular convolution and the
depthwise separable convolution operation (a depthwise convolution followed by
a pointwise convolution). In this light, a depthwise separable convolution can
be understood as an Inception module with a maximally large number of towers.
This observation leads us to propose a novel deep convolutional neural network
architecture inspired by Inception, where Inception modules have been replaced
with depthwise separable convolutions. We show that this architecture, dubbed
Xception, slightly outperforms Inception V3 on the ImageNet dataset (which
Inception V3 was designed for), and significantly outperforms Inception V3 on a
larger image classification dataset comprising 350 million images and 17,000
classes. Since the Xception architecture has the same number of parameters as
Inception V3, the performance gains are not due to increased capacity but
rather to a more efficient use of model parameters.},
 archiveprefix = {arXiv},
 author = {François Chollet},
 eprint = {1610.02357v3},
 file = {1610.02357v3.pdf},
 month = {Nov},
 primaryclass = {cs.CV},
 title = {Xception: Deep Learning with Depthwise Separable Convolutions},
 url = {https://arxiv.org/abs/1610.02357v3},
 year = {2016}
}


@article{sLPsrfbl,
 abstract = {Melanoma is the deadliest form of skin cancer. While curable with early
detection, only highly trained specialists are capable of accurately
recognizing the disease. As expertise is in limited supply, automated systems
capable of identifying disease could save lives, reduce unnecessary biopsies, and reduce costs. Toward this goal, we propose a system that combines recent
developments in deep learning with established machine learning approaches, creating ensembles of methods that are capable of segmenting skin lesions, as
well as analyzing the detected area and surrounding tissue for melanoma
detection. The system is evaluated using the largest publicly available
benchmark dataset of dermoscopic images, containing 900 training and 379
testing images. New state-of-the-art performance levels are demonstrated, leading to an improvement in the area under receiver operating characteristic
curve of 7.5% (0.843 vs. 0.783), in average precision of 4% (0.649 vs. 0.624), and in specificity measured at the clinically relevant 95% sensitivity
operating point 2.9 times higher than the previous state-of-the-art (36.8%
specificity compared to 12.5%). Compared to the average of 8 expert
dermatologists on a subset of 100 test images, the proposed system produces a
higher accuracy (76% vs. 70.5%), and specificity (62% vs. 59%) evaluated at an
equivalent sensitivity (82%).},
 archiveprefix = {arXiv},
 author = {Noel Codella and Quoc-Bao Nguyen and Sharath Pankanti and David Gutman and Brian Helba and Allan Halpern and John R. Smith},
 eprint = {1610.04662v2},
 file = {1610.04662v2.pdf},
 month = {Nov},
 note = {IBM Journal of Research and Development, vol. 61, no. 4/5, 2017},
 primaryclass = {cs.CV},
 title = {Deep Learning Ensembles for Melanoma Recognition in Dermoscopy Images},
 url = {https://arxiv.org/abs/1610.04662v2},
 year = {2016}
}


@article{YwdqeYZi,
 abstract = {We present a library of efficient implementations of deep learning
primitives. Deep learning workloads are computationally intensive, and
optimizing their kernels is difficult and time-consuming. As parallel
architectures evolve, kernels must be reoptimized, which makes maintaining
codebases difficult over time. Similar issues have long been addressed in the
HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).
However, there is no analogous library for deep learning. Without such a
library, researchers implementing deep learning workloads on parallel
processors must create and optimize their own implementations of the main
computational kernels, and this work must be repeated as new parallel
processors emerge. To address this problem, we have created a library similar
in intent to BLAS, with optimized routines for deep learning workloads. Our
implementation contains routines for GPUs, although similarly to the BLAS
library, these routines could be implemented for other platforms. The library
is easy to integrate into existing frameworks, and provides optimized
performance and memory usage. For example, integrating cuDNN into Caffe, a
popular framework for convolutional networks, improves performance by 36% on a
standard model while also reducing memory consumption.},
 archiveprefix = {arXiv},
 author = {Sharan Chetlur and Cliff Woolley and Philippe Vandermersch and Jonathan Cohen and John Tran and Bryan Catanzaro and Evan Shelhamer},
 eprint = {1410.0759v3},
 file = {1410.0759v3.pdf},
 month = {Nov},
 primaryclass = {cs.NE},
 title = {cuDNN: Efficient Primitives for Deep Learning},
 url = {https://arxiv.org/abs/1410.0759v3},
 year = {2014}
}


@article{1Dzz0P0qr,
 abstract = {Although artificial neural networks have occasionally been used for
Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in
the past, the literature has of late been dominated by other machine learning
techniques such as random forests. However, a variety of new neural net
techniques along with successful applications in other domains have renewed
interest in network approaches. In this work, inspired by the winning team's
use of neural networks in a recent QSAR competition, we used an artificial
neural network to learn a function that predicts activities of compounds for
multiple assays at the same time. We conducted experiments leveraging recent
methods for dealing with overfitting in neural networks as well as other tricks
from the neural networks literature. We compared our methods to alternative
methods reported to perform well on these tasks and found that our neural net
methods provided superior performance.},
 archiveprefix = {arXiv},
 author = {George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov},
 eprint = {1406.1231v1},
 file = {1406.1231v1.pdf},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Multi-task Neural Networks for QSAR Predictions},
 url = {https://arxiv.org/abs/1406.1231v1},
 year = {2014}
}


@article{SAvEOARL,
 abstract = {Each human genome is a 3 billion base pair set of encoding instructions.
Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design
architectures to suit it. As such, architectures that fit the structure of
genomics should be learned not prescribed. Here, we develop a novel search
algorithm, applicable across domains, that discovers an optimal architecture
which simultaneously learns general genomic patterns and identifies the most
important sequence motifs in predicting functional genomic outcomes. The
architectures we find using this algorithm succeed at using only RNA expression
data to predict gene regulatory structure, learn human-interpretable
visualizations of key sequence motifs, and surpass state-of-the-art results on
benchmark genomics challenges.},
 archiveprefix = {arXiv},
 author = {Laura Deming and Sasha Targ and Nate Sauder and Diogo Almeida and Chun Jimmie Ye},
 eprint = {1605.07156v1},
 file = {1605.07156v1.pdf},
 month = {May},
 primaryclass = {cs.LG},
 title = {Genetic Architect: Discovering Genomic Structure with Learned Neural
Architectures},
 url = {https://arxiv.org/abs/1605.07156v1},
 year = {2016}
}


@article{y4t9EzPn,
 abstract = {As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, e.g. problems in health, it is critical that researchers can
explain how such algorithms arrived at their predictions. In recent years, a
number of image saliency methods have been developed to summarize where highly
complex neural networks "look" in an image for evidence for their predictions.
However, these techniques are limited by their heuristic nature and
architectural constraints.
In this paper, we make two main contributions: First, we propose a general
framework for learning different kinds of explanations for any black box
algorithm. Second, we introduce a paradigm that learns the minimally salient
part of an image by directly editing it and learning from the corresponding
changes to its output. Unlike previous works, our method is model-agnostic and
testable because it is grounded in replicable image perturbations.},
 archiveprefix = {arXiv},
 author = {Ruth Fong and Andrea Vedaldi},
 eprint = {1704.03296v1},
 file = {1704.03296v1.pdf},
 month = {Apr},
 primaryclass = {cs.CV},
 title = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
 url = {https://arxiv.org/abs/1704.03296v1},
 year = {2017}
}


@article{1FDihfnM,
 abstract = {Deep learning tools have gained tremendous attention in applied machine
learning. However such tools for regression and classification do not capture
model uncertainty. In comparison, Bayesian models offer a mathematically
grounded framework to reason about model uncertainty, but usually come with a
prohibitive computational cost. In this paper we develop a new theoretical
framework casting dropout training in deep neural networks (NNs) as approximate
Bayesian inference in deep Gaussian processes. A direct result of this theory
gives us tools to model uncertainty with dropout NNs -- extracting information
from existing models that has been thrown away so far. This mitigates the
problem of representing uncertainty in deep learning without sacrificing either
computational complexity or test accuracy. We perform an extensive study of the
properties of dropout's uncertainty. Various network architectures and
non-linearities are assessed on tasks of regression and classification, using
MNIST as an example. We show a considerable improvement in predictive
log-likelihood and RMSE compared to existing state-of-the-art methods, and
finish by using dropout's uncertainty in deep reinforcement learning.},
 archiveprefix = {arXiv},
 author = {Yarin Gal and Zoubin Ghahramani},
 eprint = {1506.02142v6},
 file = {1506.02142v6.pdf},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in
Deep Learning},
 url = {https://arxiv.org/abs/1506.02142v6},
 year = {2015}
}


@article{2dU8f4XJ,
 abstract = {We report a method to convert discrete representations of molecules to and
from a multidimensional continuous representation. This generative model allows
efficient search and optimization through open-ended spaces of chemical
compounds. We train deep neural networks on hundreds of thousands of existing
chemical structures to construct two coupled functions: an encoder and a
decoder. The encoder converts the discrete representation of a molecule into a
real-valued continuous vector, and the decoder converts these continuous
vectors back to the discrete representation from this latent space. Continuous
representations allow us to automatically generate novel chemical structures by
performing simple operations in the latent space, such as decoding random
vectors, perturbing known chemical structures, or interpolating between
molecules. Continuous representations also allow the use of powerful
gradient-based optimization to efficiently guide the search for optimized
functional compounds. We demonstrate our method in the design of drug-like
molecules as well as organic light-emitting diodes.},
 archiveprefix = {arXiv},
 author = {Rafael Gómez-Bombarelli and David Duvenaud and José Miguel Hernández-Lobato and Jorge Aguilera-Iparraguirre and Timothy D. Hirzel and Ryan P. Adams and Alán Aspuru-Guzik},
 eprint = {1610.02415v2},
 file = {1610.02415v2.pdf},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Automatic chemical design using a data-driven continuous representation
of molecules},
 url = {https://arxiv.org/abs/1610.02415v2},
 year = {2016}
}


@article{CKcJuj03,
 abstract = {Training of large-scale deep neural networks is often constrained by the
available computational resources. We study the effect of limited precision
data representation and computation on neural network training. Within the
context of low-precision fixed-point computations, we observe the rounding
scheme to play a crucial role in determining the network's behavior during
training. Our results show that deep networks can be trained using only 16-bit
wide fixed-point number representation when using stochastic rounding, and
incur little to no degradation in the classification accuracy. We also
demonstrate an energy-efficient hardware accelerator that implements
low-precision fixed-point arithmetic with stochastic rounding.},
 archiveprefix = {arXiv},
 author = {Suyog Gupta and Ankur Agrawal and Kailash Gopalakrishnan and Pritish Narayanan},
 eprint = {1502.02551v1},
 file = {1502.02551v1.pdf},
 month = {Feb},
 primaryclass = {cs.LG},
 title = {Deep Learning with Limited Numerical Precision},
 url = {https://arxiv.org/abs/1502.02551v1},
 year = {2015}
}


@article{13KjSCKB2,
 abstract = {We present Caffe con Troll (CcT), a fully compatible end-to-end version of
the popular framework Caffe with rebuilt internals. We built CcT to examine the
performance characteristics of training and deploying general-purpose
convolutional neural networks across different hardware architectures. We find
that, by employing standard batching optimizations for CPU training, we achieve
a 4.5x throughput improvement over Caffe on popular networks like CaffeNet.
Moreover, with these improvements, the end-to-end training time for CNNs is
directly proportional to the FLOPS delivered by the CPU, which enables us to
efficiently train hybrid CPU-GPU systems for CNNs.},
 archiveprefix = {arXiv},
 author = {Stefan Hadjis and Firas Abuzaid and Ce Zhang and Christopher Ré},
 eprint = {1504.04343v2},
 file = {1504.04343v2.pdf},
 month = {Apr},
 primaryclass = {cs.LG},
 title = {Caffe con Troll: Shallow Ideas to Speed Up Deep Learning},
 url = {https://arxiv.org/abs/1504.04343v2},
 year = {2015}
}


@article{j7KrVyi8,
 abstract = {Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.},
 archiveprefix = {arXiv},
 author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
 eprint = {1512.03385v1},
 file = {1512.03385v1.pdf},
 month = {12},
 primaryclass = {cs.CV},
 title = {Deep Residual Learning for Image Recognition},
 url = {https://arxiv.org/abs/1512.03385v1},
 year = {2015}
}


@article{1CRF3gAV,
 abstract = {A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.},
 archiveprefix = {arXiv},
 author = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
 eprint = {1503.02531v1},
 file = {1503.02531v1.pdf},
 month = {Mar},
 primaryclass = {stat.ML},
 title = {Distilling the Knowledge in a Neural Network},
 url = {https://arxiv.org/abs/1503.02531v1},
 year = {2015}
}


@article{1GUizyE8e,
 abstract = {We introduce a method to train Quantized Neural Networks (QNNs) --- neural
networks with extremely low precision (e.g., 1-bit) weights and activations, at
run-time. At train-time the quantized weights and activations are used for
computing the parameter gradients. During the forward pass, QNNs drastically
reduce memory size and accesses, and replace most arithmetic operations with
bit-wise operations. As a result, power consumption is expected to be
drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and
ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to
their 32-bit counterparts. For example, our quantized version of AlexNet with
1-bit weights and 2-bit activations achieves $51\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients
computation using only bit-wise operation. Quantized recurrent neural networks
were tested over the Penn Treebank dataset, and achieved comparable accuracy as
their 32-bit counterparts using only 4-bits. Last but not least, we programmed
a binary matrix multiplication GPU kernel with which it is possible to run our
MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering
any loss in classification accuracy. The QNN code is available online.},
 archiveprefix = {arXiv},
 author = {Itay Hubara and Matthieu Courbariaux and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
 eprint = {1609.07061v1},
 file = {1609.07061v1.pdf},
 month = {Sep},
 primaryclass = {cs.NE},
 title = {Quantized Neural Networks: Training Neural Networks with Low Precision
Weights and Activations},
 url = {https://arxiv.org/abs/1609.07061v1},
 year = {2016}
}


@article{1AJUcl1KV,
 abstract = {Melanoma is amongst most aggressive types of cancer. However, it is highly
curable if detected in its early stages. Prescreening of suspicious moles and
lesions for malignancy is of great importance. Detection can be done by images
captured by standard cameras, which are more preferable due to low cost and
availability. One important step in computerized evaluation of skin lesions is
accurate detection of lesion region, i.e. segmentation of an image into two
regions as lesion and normal skin. Accurate segmentation can be challenging due
to burdens such as illumination variation and low contrast between lesion and
healthy skin. In this paper, a method based on deep neural networks is proposed
for accurate extraction of a lesion region. The input image is preprocessed and
then its patches are fed to a convolutional neural network (CNN). Local texture
and global structure of the patches are processed in order to assign pixels to
lesion or normal classes. A method for effective selection of training patches
is used for more accurate detection of a lesion border. The output segmentation
mask is refined by some post processing operations. The experimental results of
qualitative and quantitative evaluations demonstrate that our method can
outperform other state-of-the-art algorithms exist in the literature.},
 archiveprefix = {arXiv},
 author = {Mohammad H. Jafari and Ebrahim Nasr-Esfahani and Nader Karimi and S. M. Reza Soroushmehr and Shadrokh Samavi and Kayvan Najarian},
 doi = {10.1007/s11548-017-1567-8},
 eprint = {1609.02374v1},
 file = {1609.02374v1.pdf},
 month = {Sep},
 primaryclass = {cs.CV},
 title = {Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep
Learning},
 url = {https://arxiv.org/abs/1609.02374v1},
 year = {2016}
}


@article{71c6rs2z,
 abstract = {We present a conditional generative model to learn variation in cell and
nuclear morphology and the location of subcellular structures from microscopy
images. Our model generalizes to a wide range of subcellular localization and
allows for a probabilistic interpretation of cell and nuclear morphology and
structure localization from fluorescence images. We demonstrate the
effectiveness of our approach by producing photo-realistic cell images using
our generative model. The conditional nature of the model provides the ability
to predict the localization of unobserved structures given cell and nuclear
morphology.},
 archiveprefix = {arXiv},
 author = {Gregory R. Johnson and Rory M. Donovan-Maiye and Mary M. Maleckar},
 eprint = {1705.00092v1},
 file = {1705.00092v1.pdf},
 month = {Apr},
 primaryclass = {stat.ML},
 title = {Generative Modeling with Conditional Autoencoders: Building an
Integrated Cell},
 url = {https://arxiv.org/abs/1705.00092v1},
 year = {2017}
}


@article{QphVo2P2,
 abstract = {While deep learning models have achieved state-of-the-art accuracies for many
prediction tasks, understanding these models remains a challenge. Despite the
recent interest in developing visual tools to help users interpret deep
learning models, the complexity and wide variety of models deployed in
industry, and the large-scale datasets that they used, pose unique design
challenges that are inadequately addressed by existing work. Through
participatory design sessions with over 15 researchers and engineers at
Facebook, we have developed, deployed, and iteratively improved ActiVis, an
interactive visualization system for interpreting large-scale deep learning
models and results. By tightly integrating multiple coordinated views, such as
a computation graph overview of the model architecture, and a neuron activation
view for pattern discovery and comparison, users can explore complex deep
neural network models at both the instance- and subset-level. ActiVis has been
deployed on Facebook's machine learning platform. We present case studies with
Facebook researchers and engineers, and usage scenarios of how ActiVis may work
with different models.},
 archiveprefix = {arXiv},
 author = {Minsuk Kahng and Pierre Andrews and Aditya Kalro and Duen Horng Chau},
 eprint = {1704.01942v1},
 file = {1704.01942v1.pdf},
 month = {Apr},
 primaryclass = {cs.HC},
 title = {ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models},
 url = {https://arxiv.org/abs/1704.01942v1},
 year = {2017}
}


@article{2cpYveR4,
 abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long
Short-Term Memory (LSTM), are enjoying renewed interest as a result of
successful applications in a wide range of machine learning problems that
involve sequential data. However, while LSTMs provide exceptional results in
practice, the source of their performance and their limitations remain rather
poorly understood. Using character-level language models as an interpretable
testbed, we aim to bridge this gap by providing an analysis of their
representations, predictions and error types. In particular, our experiments
reveal the existence of interpretable cells that keep track of long-range
dependencies such as line lengths, quotes and brackets. Moreover, our
comparative analysis with finite horizon n-gram models traces the source of the
LSTM improvements to long-range structural dependencies. Finally, we provide
analysis of the remaining errors and suggests areas for further study.},
 archiveprefix = {arXiv},
 author = {Andrej Karpathy and Justin Johnson and Li Fei-Fei},
 eprint = {1506.02078v2},
 file = {1506.02078v2.pdf},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Visualizing and Understanding Recurrent Networks},
 url = {https://arxiv.org/abs/1506.02078v2},
 year = {2015}
}


@article{uP7SgBVd,
 abstract = {Deep learning methods such as multitask neural networks have recently been
applied to ligand-based virtual screening and other drug discovery
applications. Using a set of industrial ADMET datasets, we compare neural
networks to standard baseline models and analyze multitask learning effects
with both random cross-validation and a more relevant temporal validation
scheme. We confirm that multitask learning can provide modest benefits over
single-task models and show that smaller datasets tend to benefit more than
larger datasets from multitask learning. Additionally, we find that adding
massive amounts of side information is not guaranteed to improve performance
relative to simpler multitask learning. Our results emphasize that multitask
effects are highly dataset-dependent, suggesting the use of dataset-specific
models to maximize overall performance.},
 archiveprefix = {arXiv},
 author = {Steven Kearnes and Brian Goldman and Vijay Pande},
 eprint = {1606.08793v3},
 file = {1606.08793v3.pdf},
 month = {Jun},
 primaryclass = {stat.ML},
 title = {Modeling Industrial ADMET Data with Multitask Networks},
 url = {https://arxiv.org/abs/1606.08793v3},
 year = {2016}
}


@article{b1sc0cgP,
 abstract = {Understanding neural networks is becoming increasingly important. Over the
last few years different types of visualisation and explanation methods have
been proposed. However, none of them explicitly considered the behaviour in the
presence of noise and distracting elements. In this work, we will show how
noise and distracting dimensions can influence the result of an explanation
model. This gives a new theoretical insights to aid selection of the most
appropriate explanation model within the deep-Taylor decomposition framework.},
 archiveprefix = {arXiv},
 author = {Pieter-Jan Kindermans and Kristof Schütt and Klaus-Robert Müller and Sven Dähne},
 eprint = {1611.07270v1},
 file = {1611.07270v1.pdf},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {Investigating the influence of noise and distractors on the
interpretation of neural networks},
 url = {https://arxiv.org/abs/1611.07270v1},
 year = {2016}
}


@article{69wxD9y,
 abstract = {How can we explain the predictions of a black-box model? In this paper, we
use influence functions -- a classic technique from robust statistics -- to
trace a model's prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.},
 archiveprefix = {arXiv},
 author = {Pang Wei Koh and Percy Liang},
 eprint = {1703.04730v2},
 file = {1703.04730v2.pdf},
 month = {Mar},
 primaryclass = {stat.ML},
 title = {Understanding Black-box Predictions via Influence Functions},
 url = {https://arxiv.org/abs/1703.04730v2},
 year = {2017}
}


@article{ZSVsnPVO,
 abstract = {I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.},
 archiveprefix = {arXiv},
 author = {Alex Krizhevsky},
 eprint = {1404.5997v2},
 file = {1404.5997v2.pdf},
 month = {Apr},
 primaryclass = {cs.NE},
 title = {One weird trick for parallelizing convolutional neural networks},
 url = {https://arxiv.org/abs/1404.5997v2},
 year = {2014}
}


@article{9NKsJjSw,
 abstract = {The rapid growth of data size and accessibility in recent years has
instigated a shift of philosophy in algorithm design for artificial
intelligence. Instead of engineering algorithms by hand, the ability to learn
composable systems automatically from massive amounts of data has led to
ground-breaking performance in important domains such as computer vision, speech recognition, and natural language processing. The most popular class of
techniques used in these domains is called deep learning, and is seeing
significant attention from industry. However, these models require incredible
amounts of data and compute power to train, and are limited by the need for
better hardware acceleration to accommodate scaling beyond current data and
model sizes. While the current solution has been to use clusters of graphics
processing units (GPU) as general purpose processors (GPGPU), the use of field
programmable gate arrays (FPGA) provide an interesting alternative. Current
trends in design tools for FPGAs have made them more compatible with the
high-level software practices typically practiced in the deep learning
community, making FPGAs more accessible to those who build and deploy models.
Since FPGA architectures are flexible, this could also allow researchers the
ability to explore model-level optimizations beyond what is possible on fixed
architectures such as GPUs. As well, FPGAs tend to provide high performance per
watt of power consumption, which is of particular importance for application
scientists interested in large scale server-based deployment or
resource-limited embedded applications. This review takes a look at deep
learning and FPGAs from a hardware acceleration perspective, identifying trends
and innovations that make these technologies a natural fit, and motivates a
discussion on how FPGAs may best serve the needs of the deep learning community
moving forward.},
 archiveprefix = {arXiv},
 author = {Griffin Lacey and Graham W. Taylor and Shawki Areibi},
 eprint = {1602.04283v1},
 file = {1602.04283v1.pdf},
 month = {Feb},
 primaryclass = {cs.DC},
 title = {Deep Learning on FPGAs: Past, Present, and Future},
 url = {https://arxiv.org/abs/1602.04283v1},
 year = {2016}
}


@article{Dwi2eAvT,
 abstract = {Deep neural network (DNN) models have recently obtained state-of-the-art
prediction accuracy for the transcription factor binding (TFBS) site
classification task. However, it remains unclear how these approaches identify
meaningful DNA sequence signals and give insights as to why TFs bind to certain
locations. In this paper, we propose a toolkit called the Deep Motif Dashboard
(DeMo Dashboard) which provides a suite of visualization strategies to extract
motifs, or sequence patterns from deep neural network models for TFBS
classification. We demonstrate how to visualize and understand three important
DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our
first visualization method is finding a test sequence's saliency map which uses
first-order derivatives to describe the importance of each nucleotide in making
the final prediction. Second, considering recurrent models make predictions in
a temporal manner (from one end of a TFBS sequence to the other), we introduce
temporal output scores, indicating the prediction score of a model over time
for a sequential input. Lastly, a class-specific visualization strategy finds
the optimal input sequence for a given TFBS positive class via stochastic
gradient optimization. Our experimental results indicate that a
convolutional-recurrent architecture performs the best among the three
architectures. The visualization techniques indicate that CNN-RNN makes
predictions by modeling both motifs as well as dependencies among them.},
 archiveprefix = {arXiv},
 author = {Jack Lanchantin and Ritambhara Singh and Beilun Wang and Yanjun Qi},
 eprint = {1608.03644v4},
 file = {1608.03644v4.pdf},
 month = {Aug},
 primaryclass = {cs.LG},
 title = {Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences
Using Deep Neural Networks},
 url = {https://arxiv.org/abs/1608.03644v4},
 year = {2016}
}


@article{1GwC1ll6h,
 abstract = {MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control the
expression of target messenger RNAs (mRNAs) by binding them. Robust prediction
of miRNA-mRNA pairs is of utmost importance in deciphering gene regulations but
has been challenging because of high false positive rates, despite a deluge of
computational tools that normally require laborious manual feature extraction.
This paper presents an end-to-end machine learning framework for miRNA target
prediction. Leveraged by deep recurrent neural networks-based auto-encoding and
sequence-sequence interaction learning, our approach not only delivers an
unprecedented level of accuracy but also eliminates the need for manual feature
extraction. The performance gap between the proposed method and existing
alternatives is substantial (over 25% increase in F-measure), and deepTarget
delivers a quantum leap in the long-standing challenge of robust miRNA target
prediction.},
 archiveprefix = {arXiv},
 author = {Byunghan Lee and Junghwan Baek and Seunghyun Park and Sungroh Yoon},
 eprint = {1603.09123v2},
 file = {1603.09123v2.pdf},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {deepTarget: End-to-end Learning Framework for microRNA Target Prediction
using Deep Recurrent Neural Networks},
 url = {https://arxiv.org/abs/1603.09123v2},
 year = {2016}
}


@article{ZUCVI5eU,
 abstract = {Prediction without justification has limited applicability. As a remedy, we
learn to extract pieces of input text as justifications -- rationales -- that
are tailored to be short and coherent, yet sufficient for making the same
prediction. Our approach combines two modular components, generator and
encoder, which are trained to operate well together. The generator specifies a
distribution over text fragments as candidate rationales and these are passed
through the encoder for prediction. Rationales are never given during training.
Instead, the model is regularized by desiderata for rationales. We evaluate the
approach on multi-aspect sentiment analysis against manually annotated test
cases. Our approach outperforms attention-based baseline by a significant
margin. We also successfully illustrate the method on the question retrieval
task.},
 archiveprefix = {arXiv},
 author = {Tao Lei and Regina Barzilay and Tommi Jaakkola},
 eprint = {1606.04155v2},
 file = {1606.04155v2.pdf},
 month = {Jun},
 primaryclass = {cs.CL},
 title = {Rationalizing Neural Predictions},
 url = {https://arxiv.org/abs/1606.04155v2},
 year = {2016}
}


@article{glyI7H6F,
 abstract = {We present a novel application of LSTM recurrent neural networks to
multilabel classification of diagnoses given variable-length time series of
clinical measurements. Our method outperforms a strong baseline on a variety of
metrics.},
 archiveprefix = {arXiv},
 author = {Zachary C. Lipton and David C. Kale and Randall C. Wetzel},
 eprint = {1510.07641v2},
 file = {1510.07641v2.pdf},
 month = {Nov},
 primaryclass = {cs.LG},
 title = {Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks},
 url = {https://arxiv.org/abs/1510.07641v2},
 year = {2015}
}


@article{4zpZxjHR,
 abstract = {We demonstrate a simple strategy to cope with missing data in sequential
inputs, addressing the task of multilabel classification of diagnoses given
clinical time series. Collected from the pediatric intensive care unit (PICU)
at Children's Hospital Los Angeles, our data consists of multivariate time
series of observations. The measurements are irregularly spaced, leading to
missingness patterns in temporally discretized sequences. While these artifacts
are typically handled by imputation, we achieve superior predictive performance
by treating the artifacts as features. Unlike linear models, recurrent neural
networks can realize this improvement using only simple binary indicators of
missingness. For linear models, we show an alternative strategy to capture this
signal. Training models on missingness patterns only, we show that for some
diseases, what tests are run can be as predictive as the results themselves.},
 archiveprefix = {arXiv},
 author = {Zachary C. Lipton and David C. Kale and Randall Wetzel},
 eprint = {1606.04130v5},
 file = {1606.04130v5.pdf},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Modeling Missing Data in Clinical Time Series with RNNs},
 url = {https://arxiv.org/abs/1606.04130v5},
 year = {2016}
}


@article{LL5huVs3,
 abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly
become a methodology of choice for analyzing medical images. This paper reviews
the major deep learning concepts pertinent to medical image analysis and
summarizes over 300 contributions to the field, most of which appeared in the
last year. We survey the use of deep learning for image classification, object
detection, segmentation, registration, and other tasks and provide concise
overviews of studies per application area. Open challenges and directions for
future research are discussed.},
 archiveprefix = {arXiv},
 author = {Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A. W. M. van der Laak and Bram van Ginneken and Clara I. Sánchez},
 doi = {10.1016/j.media.2017.07.005},
 eprint = {1702.05747v2},
 file = {1702.05747v2.pdf},
 month = {Feb},
 primaryclass = {cs.CV},
 title = {A Survey on Deep Learning in Medical Image Analysis},
 url = {https://arxiv.org/abs/1702.05747v2},
 year = {2017}
}


@article{AEc66xxR,
 abstract = {Deep convolutional neural networks (CNNs) have achieved breakthrough
performance in many pattern recognition tasks such as image classification.
However, the development of high-quality deep models typically relies on a
substantial amount of trial-and-error, as there is still no clear understanding
of when and why a deep model works. In this paper, we present a visual
analytics approach for better understanding, diagnosing, and refining deep
CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this
formulation, a hybrid visualization is developed to disclose the multiple
facets of each neuron and the interactions between them. In particular, we
introduce a hierarchical rectangle packing algorithm and a matrix reordering
algorithm to show the derived features of a neuron cluster. We also propose a
biclustering-based edge bundling method to reduce visual clutter caused by a
large number of connections between neurons. We evaluated our method on a set
of CNNs and the results are generally favorable.},
 archiveprefix = {arXiv},
 author = {Mengchen Liu and Jiaxin Shi and Zhen Li and Chongxuan Li and Jun Zhu and Shixia Liu},
 eprint = {1604.07043v3},
 file = {1604.07043v3.pdf},
 month = {Apr},
 primaryclass = {cs.CV},
 title = {Towards Better Analysis of Deep Convolutional Neural Networks},
 url = {https://arxiv.org/abs/1604.07043v3},
 year = {2016}
}


@article{DeOI1oGf,
 abstract = {Understanding why a model made a certain prediction is crucial in many data
science fields. Interpretable predictions engender appropriate trust and
provide insight into how the model may be improved. However, with large modern
datasets the best accuracy is often achieved by complex models even experts
struggle to interpret, which creates a tension between accuracy and
interpretability. Recently, several methods have been proposed for interpreting
predictions from complex models by estimating the importance of input features.
Here, we present how a model-agnostic additive representation of the importance
of input features unifies current methods. This representation is optimal, in
the sense that it is the only set of additive values that satisfies important
properties. We show how we can leverage these properties to create novel visual
explanations of model predictions. The thread of unity that this representation
weaves through the literature indicates that there are common principles to be
learned about the interpretation of model predictions that apply in many
scenarios.},
 archiveprefix = {arXiv},
 author = {Scott Lundberg and Su-In Lee},
 eprint = {1611.07478v3},
 file = {1611.07478v3.pdf},
 month = {Dec},
 primaryclass = {cs.AI},
 title = {An unexpected unity among methods for interpreting model predictions},
 url = {https://arxiv.org/abs/1611.07478v3},
 year = {2016}
}


@article{19mGl6pfy,
 abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional
Neural Networks (CNNs), are a crucial component of almost any image
understanding system. Nevertheless, our understanding of them remains limited.
In this paper we conduct a direct analysis of the visual information contained
in representations by asking the following question: given an encoding of an
image, to which extent is it possible to reconstruct the image itself? To
answer this question we contribute a general framework to invert
representations. We show that this method can invert representations such as
HOG and SIFT more accurately than recent alternatives while being applicable to
CNNs too. We then use this technique to study the inverse of recent
state-of-the-art CNN image representations for the first time. Among our
findings, we show that several layers in CNNs retain photographically accurate
information about the image, with different degrees of geometric and
photometric invariance.},
 archiveprefix = {arXiv},
 author = {Aravindh Mahendran and Andrea Vedaldi},
 eprint = {1412.0035v1},
 file = {1412.0035v1.pdf},
 month = {Dec},
 primaryclass = {cs.CV},
 title = {Understanding Deep Image Representations by Inverting Them},
 url = {https://arxiv.org/abs/1412.0035v1},
 year = {2014}
}


@article{rZnxDitd,
 abstract = {Apache Spark is a popular open-source platform for large-scale data
processing that is well-suited for iterative machine learning tasks. In this
paper we present MLlib, Spark's open-source distributed machine learning
library. MLlib provides efficient functionality for a wide range of learning
settings and includes several underlying statistical, optimization, and linear
algebra primitives. Shipped with Spark, MLlib supports several languages and
provides a high-level API that leverages Spark's rich ecosystem to simplify the
development of end-to-end machine learning pipelines. MLlib has experienced a
rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users
quickly get up to speed.},
 archiveprefix = {arXiv},
 author = {Xiangrui Meng and Joseph Bradley and Burak Yavuz and Evan Sparks and Shivaram Venkataraman and Davies Liu and Jeremy Freeman and DB Tsai and Manish Amde and Sean Owen and Doris Xin and Reynold Xin and Michael J. Franklin and Reza Zadeh and Matei Zaharia and Ameet Talwalkar},
 eprint = {1505.06807v1},
 file = {1505.06807v1.pdf},
 month = {May},
 primaryclass = {cs.LG},
 title = {MLlib: Machine Learning in Apache Spark},
 url = {https://arxiv.org/abs/1505.06807v1},
 year = {2015}
}


@article{rmJZ2Aui,
 abstract = {Training deep networks is a time-consuming process, with networks for object
recognition often requiring multiple days to train. For this reason, leveraging
the resources of a cluster to speed up training is an important area of work.
However, widely-popular batch-processing computational frameworks like
MapReduce and Spark were not designed to support the asynchronous and
communication-intensive workloads of existing distributed deep learning
systems. We introduce SparkNet, a framework for training deep networks in
Spark. Our implementation includes a convenient interface for reading data from
Spark RDDs, a Scala interface to the Caffe deep learning framework, and a
lightweight multi-dimensional tensor library. Using a simple parallelization
scheme for stochastic gradient descent, SparkNet scales well with the cluster
size and tolerates very high-latency communication. Furthermore, it is easy to
deploy and use with no parameter tuning, and it is compatible with existing
Caffe models. We quantify the dependence of the speedup obtained by SparkNet on
the number of machines, the communication frequency, and the cluster's
communication overhead, and we benchmark our system's performance on the
ImageNet dataset.},
 archiveprefix = {arXiv},
 author = {Philipp Moritz and Robert Nishihara and Ion Stoica and Michael I. Jordan},
 eprint = {1511.06051v4},
 file = {1511.06051v4.pdf},
 month = {Dec},
 primaryclass = {stat.ML},
 title = {SparkNet: Training Deep Networks in Spark},
 url = {https://arxiv.org/abs/1511.06051v4},
 year = {2015}
}


@article{10ViHstXn,
 abstract = {Although deep learning models have proven effective at solving problems in
natural language processing, the mechanism by which they come to their
conclusions is often unclear. As a result, these models are generally treated
as black boxes, yielding no insight of the underlying learned patterns. In this
paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new
approach for tracking the importance of a given input to the LSTM for a given
output. By identifying consistently important patterns of words, we are able to
distill state of the art LSTMs on sentiment analysis and question answering
into a set of representative phrases. This representation is then
quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
 archiveprefix = {arXiv},
 author = {W. James Murdoch and Arthur Szlam},
 eprint = {1702.02540v2},
 file = {1702.02540v2.pdf},
 month = {Feb},
 primaryclass = {cs.CL},
 title = {Automatic Rule Extraction from Long Short Term Memory Networks},
 url = {https://arxiv.org/abs/1702.02540v2},
 year = {2017}
}


@article{1AkF8Wsv7,
 abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.},
 archiveprefix = {arXiv},
 author = {Anh Nguyen and Jason Yosinski and Jeff Clune},
 eprint = {1412.1897v4},
 file = {1412.1897v4.pdf},
 month = {12},
 primaryclass = {cs.CV},
 title = {Deep Neural Networks are Easily Fooled: High Confidence Predictions for
Unrecognizable Images},
 url = {https://arxiv.org/abs/1412.1897v4},
 year = {2014}
}


@article{1EayJRsI,
 abstract = {This work introduces a method to tune a sequence-based generative model for
molecular de novo design that through augmented episodic likelihood can learn
to generate structures with certain specified desirable properties. We
demonstrate how this model can execute a range of tasks such as generating
analogues to a query structure and generating compounds predicted to be active
against a biological target. As a proof of principle, the model is first
trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique
that could be used for scaffold hopping or library expansion starting from a
single molecule. Finally, when tuning the model towards generating compounds
predicted to be active against the dopamine receptor D2, the model generates
structures of which more than 95% are predicted to be active, including
experimentally confirmed actives that have not been included in either the
generative model nor the activity prediction model.},
 archiveprefix = {arXiv},
 author = {Marcus Olivecrona and Thomas Blaschke and Ola Engkvist and Hongming Chen},
 eprint = {1704.07555v1},
 file = {1704.07555v1.pdf},
 month = {Apr},
 primaryclass = {cs.AI},
 title = {Molecular De Novo Design through Deep Reinforcement Learning},
 url = {https://arxiv.org/abs/1704.07555v1},
 year = {2017}
}


@article{1TeyWffV,
 abstract = {Since microRNAs (miRNAs) play a crucial role in post-transcriptional gene
regulation, miRNA identification is one of the most essential problems in
computational biology. miRNAs are usually short in length ranging between 20
and 23 base pairs. It is thus often difficult to distinguish miRNA-encoding
sequences from other non-coding RNAs and pseudo miRNAs that have a similar
length, and most previous studies have recommended using precursor miRNAs
instead of mature miRNAs for robust detection. A great number of conventional
machine-learning-based classification methods have been proposed, but they
often have the serious disadvantage of requiring manual feature engineering, and their performance is limited as well. In this paper, we propose a novel
miRNA precursor prediction algorithm, deepMiRGene, based on recurrent neural
networks, specifically long short-term memory networks. deepMiRGene
automatically learns suitable features from the data themselves without manual
feature engineering and constructs a model that can successfully reflect
structural characteristics of precursor miRNAs. For the performance evaluation
of our approach, we have employed several widely used evaluation metrics on
three recent benchmark datasets and verified that deepMiRGene delivered
comparable performance among the current state-of-the-art tools.},
 archiveprefix = {arXiv},
 author = {Seunghyun Park and Seonwoo Min and Hyunsoo Choi and Sungroh Yoon},
 eprint = {1605.00017v1},
 file = {1605.00017v1.pdf},
 month = {Apr},
 primaryclass = {cs.LG},
 title = {deepMiRGene: Deep Neural Network based Precursor microRNA Prediction},
 url = {https://arxiv.org/abs/1605.00017v1},
 year = {2016}
}


@article{bNBiIiTt,
 abstract = {Computational approaches to drug discovery can reduce the time and cost
associated with experimental assays and enable the screening of novel
chemotypes. Structure-based drug design methods rely on scoring functions to
rank and predict binding affinities and poses. The ever-expanding amount of
protein-ligand binding and structural data enables the use of deep machine
learning techniques for protein-ligand scoring.
We describe convolutional neural network (CNN) scoring functions that take as
input a comprehensive 3D representation of a protein-ligand interaction. A CNN
scoring function automatically learns the key features of protein-ligand
interactions that correlate with binding. We train and optimize our CNN scoring
functions to discriminate between correct and incorrect binding poses and known
binders and non-binders. We find that our CNN scoring function outperforms the
AutoDock Vina scoring function when ranking poses both for pose prediction and
virtual screening.},
 archiveprefix = {arXiv},
 author = {Matthew Ragoza and Joshua Hochuli and Elisa Idrobo and Jocelyn Sunseri and David Ryan Koes},
 eprint = {1612.02751v1},
 file = {1612.02751v1.pdf},
 month = {12},
 primaryclass = {stat.ML},
 title = {Protein-Ligand Scoring with Convolutional Neural Networks},
 url = {https://arxiv.org/abs/1612.02751v1},
 year = {2016}
}


@article{yAoN5gTU,
 abstract = {Massively multitask neural architectures provide a learning framework for
drug discovery that synthesizes information from many distinct biological
sources. To train these architectures at scale, we gather large amounts of data
from public sources to create a dataset of nearly 40 million measurements
across more than 200 biological targets. We investigate several aspects of the
multitask framework by performing a series of empirical studies and obtain some
interesting results: (1) massively multitask networks obtain predictive
accuracies significantly better than single-task methods, (2) the predictive
power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute
significantly to multitask improvement, and (4) multitask networks afford
limited transferability to tasks not in the training set. Our results
underscore the need for greater data sharing and further algorithmic innovation
to accelerate the drug discovery process.},
 archiveprefix = {arXiv},
 author = {Bharath Ramsundar and Steven Kearnes and Patrick Riley and Dale Webster and David Konerding and Vijay Pande},
 eprint = {1502.02072v1},
 file = {1502.02072v1.pdf},
 month = {Feb},
 primaryclass = {stat.ML},
 title = {Massively Multitask Networks for Drug Discovery},
 url = {https://arxiv.org/abs/1502.02072v1},
 year = {2015}
}


@article{QwXSJhr0,
 abstract = {Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.},
 archiveprefix = {arXiv},
 author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
 eprint = {1602.04938v3},
 file = {1602.04938v3.pdf},
 month = {Feb},
 primaryclass = {cs.LG},
 title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
 url = {https://arxiv.org/abs/1602.04938v3},
 year = {2016}
}


@article{w6CoVmFK,
 abstract = {Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of
machine learning problems. Researchers and industry have developed several
techniques to optimize SGD's runtime performance, including asynchronous
execution and reduced precision. Our main result is a martingale-based analysis
that enables us to capture the rich noise models that may arise from such
techniques. Specifically, we use our new analysis in three ways: (1) we derive
convergence rates for the convex case (Hogwild!) with relaxed assumptions on
the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for
non-convex matrix problems including matrix completion; and (3) we design and
analyze an asynchronous SGD algorithm, called Buckwild!, that uses
lower-precision arithmetic. We show experimentally that our algorithms run
efficiently for a variety of problems on modern hardware.},
 archiveprefix = {arXiv},
 author = {Christopher De Sa and Ce Zhang and Kunle Olukotun and Christopher Ré},
 eprint = {1506.06438v2},
 file = {1506.06438v2.pdf},
 month = {Jun},
 primaryclass = {cs.LG},
 title = {Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms},
 url = {https://arxiv.org/abs/1506.06438v2},
 year = {2015}
}


@article{8LWFFeYg,
 abstract = {In de novo drug design, computational strategies are used to generate novel
molecules with good affinity to the desired biological target. In this work, we
show that recurrent neural networks can be trained as generative models for
molecular structures, similar to statistical language models in natural
language processing. We demonstrate that the properties of the generated
molecules correlate very well with the properties of the molecules used to
train the model. In order to enrich libraries with molecules active towards a
given biological target, we propose to fine-tune the model with small sets of
molecules, which are known to be active against that target.
Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test
molecules that medicinal chemists designed, whereas against Plasmodium
falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled
with a scoring function, our model can perform the complete de novo drug design
cycle to generate large sets of novel molecules for drug discovery.},
 archiveprefix = {arXiv},
 author = {Marwin H. S. Segler and Thierry Kogej and Christian Tyrchan and Mark P. Waller},
 eprint = {1701.01329v1},
 file = {1701.01329v1.pdf},
 month = {Jan},
 primaryclass = {cs.NE},
 title = {Generating Focussed Molecule Libraries for Drug Discovery with Recurrent
Neural Networks},
 url = {https://arxiv.org/abs/1701.01329v1},
 year = {2017}
}


@article{RZsNSRDS,
 abstract = {We propose a technique for producing "visual explanations" for decisions from
a large class of CNN-based models, making them more transparent. Our approach -
Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of
any target concept, flowing into the final convolutional layer to produce a
coarse localization map highlighting the important regions in the image for
predicting the concept. Unlike previous approaches, GradCAM is applicable to a
wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.
VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in
tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any
architectural changes or re-training. We combine GradCAM with fine-grained
visualizations to create a high-resolution class-discriminative visualization
and apply it to off-the-shelf image classification, captioning, and visual
question answering (VQA) models, including ResNet-based architectures. In the
context of image classification models, our visualizations (a) lend insights
into their failure modes (showing that seemingly unreasonable predictions have
reasonable explanations), (b) are robust to adversarial images, (c) outperform
previous methods on weakly-supervised localization, (d) are more faithful to
the underlying model and (e) help achieve generalization by identifying dataset
bias. For captioning and VQA, our visualizations show that even non-attention
based models can localize inputs. Finally, we conduct human studies to measure
if GradCAM explanations help users establish trust in predictions from deep
networks and show that GradCAM helps untrained users successfully discern a
"stronger" deep network from a "weaker" one. Our code is available at
https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found
at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
 archiveprefix = {arXiv},
 author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
 eprint = {1610.02391v3},
 file = {1610.02391v3.pdf},
 month = {Nov},
 primaryclass = {cs.CV},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based
Localization},
 url = {https://arxiv.org/abs/1610.02391v3},
 year = {2016}
}


@article{T2Md9xLY,
 abstract = {Sources of variability in experimentally derived data include measurement
error in addition to the physical phenomena of interest. This measurement error
is a combination of systematic components, originating from the measuring
instrument, and random measurement errors. Several novel biological
technologies, such as mass cytometry and single-cell RNA-seq, are plagued with
systematic errors that may severely affect statistical analysis if the data is
not properly calibrated. We propose a novel deep learning approach for removing
systematic batch effects. Our method is based on a residual network, trained to
minimize the Maximum Mean Discrepancy (MMD) between the multivariate
distributions of two replicates, measured in different batches. We apply our
method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that
it effectively attenuates batch effects.},
 archiveprefix = {arXiv},
 author = {Uri Shaham and Kelly P. Stanton and Jun Zhao and Huamin Li and Khadir Raddassi and Ruth Montgomery and Yuval Kluger},
 doi = {10.1093/bioinformatics/btx196},
 eprint = {1610.04181v5},
 file = {1610.04181v5.pdf},
 month = {Nov},
 primaryclass = {stat.ML},
 title = {Removal of Batch Effects using Distribution-Matching Residual Networks},
 url = {https://arxiv.org/abs/1610.04181v5},
 year = {2016}
}


@article{zhmq9ktJ,
 abstract = {The purported "black box"' nature of neural networks is a barrier to adoption
in applications where interpretability is essential. Here we present DeepLIFT
(Deep Learning Important FeaTures), a method for decomposing the output
prediction of a neural network on a specific input by backpropagating the
contributions of all neurons in the network to every feature of the input.
DeepLIFT compares the activation of each neuron to its 'reference activation'
and assigns contribution scores according to the difference. By optionally
giving separate consideration to positive and negative contributions, DeepLIFT
can also reveal dependencies which are missed by other approaches. Scores can
be computed efficiently in a single backward pass. We apply DeepLIFT to models
trained on MNIST and simulated genomic data, and show significant advantages
over gradient-based methods. A detailed video tutorial on the method is at
http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
 archiveprefix = {arXiv},
 author = {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
 eprint = {1704.02685v1},
 file = {1704.02685v1.pdf},
 month = {Apr},
 primaryclass = {cs.CV},
 title = {Learning Important Features Through Propagating Activation Differences},
 url = {https://arxiv.org/abs/1704.02685v1},
 year = {2017}
}


@article{1YcKYTvO,
 abstract = {This paper addresses the visualisation of image classification models, learnt
using deep Convolutional Networks (ConvNets). We consider two visualisation
techniques, based on computing the gradient of the class score with respect to
the input image. The first one generates an image, which maximises the class
score [Erhan et al., 2009], thus visualising the notion of the class, captured
by a ConvNet. The second technique computes a class saliency map, specific to a
given image and class. We show that such maps can be employed for weakly
supervised object segmentation using classification ConvNets. Finally, we
establish the connection between the gradient-based ConvNet visualisation
methods and deconvolutional networks [Zeiler et al., 2013].},
 archiveprefix = {arXiv},
 author = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
 eprint = {1312.6034v2},
 file = {1312.6034v2.pdf},
 month = {12},
 primaryclass = {cs.CV},
 title = {Deep Inside Convolutional Networks: Visualising Image Classification
Models and Saliency Maps},
 url = {https://arxiv.org/abs/1312.6034v2},
 year = {2013}
}


@article{G10wkFHt,
 abstract = {Motivation: Histone modifications are among the most important factors that
control gene regulation. Computational methods that predict gene expression
from histone modification signals are highly desirable for understanding their
combinatorial effects in gene regulation. This knowledge can help in developing
'epigenetic drugs' for diseases like cancer. Previous studies for quantifying
the relationship between histone modifications and gene expression levels
either failed to capture combinatorial effects or relied on multiple methods
that separate predictions and combinatorial analysis. This paper develops a
unified discriminative framework using a deep convolutional neural network to
classify gene expression using histone modification data as input. Our system, called DeepChrome, allows automatic extraction of complex interactions among
important features. To simultaneously visualize the combinatorial interactions
among histone modifications, we propose a novel optimization-based technique
that generates feature pattern maps from the learnt deep model. This provides
an intuitive description of underlying epigenetic mechanisms that regulate
genes. Results: We show that DeepChrome outperforms state-of-the-art models
like Support Vector Machines and Random Forests for gene expression
classification task on 56 different cell-types from REMC database. The output
of our visualization technique not only validates the previous observations but
also allows novel insights about combinatorial interactions among histone
modification marks, some of which have recently been observed by experimental
studies.},
 archiveprefix = {arXiv},
 author = {Ritambhara Singh and Jack Lanchantin and Gabriel Robins and Yanjun Qi},
 eprint = {1607.02078v1},
 file = {1607.02078v1.pdf},
 month = {Jul},
 primaryclass = {cs.LG},
 title = {DeepChrome: Deep-learning for predicting gene expression from histone
modifications},
 url = {https://arxiv.org/abs/1607.02078v1},
 year = {2016}
}


@article{81Cl5QSM,
 abstract = {Machine learning is widely used to analyze biological sequence data.
Non-sequential models such as SVMs or feed-forward neural networks are often
used although they have no natural way of handling sequences of varying length.
Recurrent neural networks such as the long short term memory (LSTM) model on
the other hand are designed to handle sequences. In this study we demonstrate
that LSTM networks predict the subcellular location of proteins given only the
protein sequence with high accuracy (0.902) outperforming current state of the
art algorithms. We further improve the performance by introducing convolutional
filters and experiment with an attention mechanism which lets the LSTM focus on
specific parts of the protein. Lastly we introduce new visualizations of both
the convolutional filters and the attention mechanisms and show how they can be
used to extract biological relevant knowledge from the LSTM networks.},
 archiveprefix = {arXiv},
 author = {Søren Kaae Sønderby and Casper Kaae Sønderby and Henrik Nielsen and Ole Winther},
 doi = {10.1007/978-3-319-21233-3_6},
 eprint = {1503.01919v1},
 file = {1503.01919v1.pdf},
 month = {Mar},
 note = {Algorithms for Computational Biology 9199 (2015) 68},
 primaryclass = {q-bio.QM},
 title = {Convolutional LSTM Networks for Subcellular Localization of Proteins},
 url = {https://arxiv.org/abs/1503.01919v1},
 year = {2015}
}


@article{f2L6isRj,
 abstract = {Most modern convolutional neural networks (CNNs) used for object recognition
are built using the same principles: Alternating convolution and max-pooling
layers followed by a small number of fully connected layers. We re-evaluate the
state of the art for object recognition from small images with convolutional
networks, questioning the necessity of different components in the pipeline. We
find that max-pooling can simply be replaced by a convolutional layer with
increased stride without loss in accuracy on several image recognition
benchmarks. Following this finding -- and building on other recent work for
finding simple network structures -- we propose a new architecture that
consists solely of convolutional layers and yields competitive or state of the
art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the
"deconvolution approach" for visualizing features learned by CNNs, which can be
applied to a broader range of network structures than existing approaches.},
 archiveprefix = {arXiv},
 author = {Jost Tobias Springenberg and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
 eprint = {1412.6806v3},
 file = {1412.6806v3.pdf},
 month = {12},
 primaryclass = {cs.LG},
 title = {Striving for Simplicity: The All Convolutional Net},
 url = {https://arxiv.org/abs/1412.6806v3},
 year = {2014}
}


@article{1Ad3UOefc,
 abstract = {Recurrent neural networks, and in particular long short-term memory networks
(LSTMs), are a remarkably effective tool for sequence modeling that learn a
dense black-box hidden representation of their sequential input. Researchers
interested in better understanding these models have studied the changes in
hidden state representations over time and noticed some interpretable patterns
but also significant noise. In this work, we present LSTMVis a visual analysis
tool for recurrent neural networks with a focus on understanding these hidden
state dynamics. The tool allows a user to select a hypothesis input range to
focus on local state changes, to match these states changes to similar patterns
in a large data set, and to align these results with domain specific structural
annotations. We further show several use cases of the tool for analyzing
specific hidden state properties on datasets containing nesting, phrase
structure, and chord progressions, and demonstrate how the tool can be used to
isolate patterns for further statistical analysis.},
 archiveprefix = {arXiv},
 author = {Hendrik Strobelt and Sebastian Gehrmann and Bernd Huber and Hanspeter Pfister and Alexander M. Rush},
 eprint = {1606.07461v1},
 file = {1606.07461v1.pdf},
 month = {Jun},
 primaryclass = {cs.CL},
 title = {Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks},
 url = {https://arxiv.org/abs/1606.07461v1},
 year = {2016}
}


@article{aClNvbyM,
 abstract = {In this work we apply model averaging to parallel training of deep neural
network (DNN). Parallelization is done in a model averaging manner. Data is
partitioned and distributed to different nodes for local model updates, and
model averaging across nodes is done every few minibatches. We use multiple
GPUs for data parallelization, and Message Passing Interface (MPI) for
communication between nodes, which allows us to perform model averaging
frequently without losing much time on communication. We investigate the
effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and
Restricted Boltzmann Machine (RBM) pretraining for parallel training in
model-averaging framework, and explore the best setups in term of different
learning rate schedules, averaging frequencies and minibatch sizes. It is shown
that NG-SGD and RBM pretraining benefits parameter-averaging based model
training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved
using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy
loss.},
 archiveprefix = {arXiv},
 author = {Hang Su and Haoyu Chen},
 eprint = {1507.01239v2},
 file = {1507.01239v2.pdf},
 month = {Jul},
 primaryclass = {cs.LG},
 title = {Experiments on Parallel Training of Deep Neural Network using Model
Averaging},
 url = {https://arxiv.org/abs/1507.01239v2},
 year = {2015}
}


@article{JUF9VoRD,
 abstract = {Parallelization framework has become a necessity to speed up the training of
deep neural networks (DNN) recently. Such framework typically employs the Model
Average approach, denoted as MA-DNN, in which parallel workers conduct
respective training based on their own local data while the parameters of local
models are periodically communicated and averaged to obtain a global model
which serves as the new start of local models. However, since DNN is a highly
non-convex model, averaging parameters cannot ensure that such global model can
perform better than those local models. To tackle this problem, we introduce a
new parallel training framework called Ensemble-Compression, denoted as EC-DNN.
In this framework, we propose to aggregate the local models by ensemble, i.e., averaging the outputs of local models instead of the parameters. As most of
prevalent loss functions are convex to the output of DNN, the performance of
ensemble-based global model is guaranteed to be at least as good as the average
performance of local models. However, a big challenge lies in the explosion of
model size since each round of ensemble can give rise to multiple times size
increment. Thus, we carry out model compression after each ensemble, specialized by a distillation based method in this paper, to reduce the size of
the global model to be the same as the local ones. Our experimental results
demonstrate the prominent advantage of EC-DNN over MA-DNN in terms of both
accuracy and speedup.},
 archiveprefix = {arXiv},
 author = {Shizhao Sun and Wei Chen and Jiang Bian and Xiaoguang Liu and Tie-Yan Liu},
 eprint = {1606.00575v2},
 file = {1606.00575v2.pdf},
 month = {Jun},
 primaryclass = {cs.DC},
 title = {Ensemble-Compression: A New Method for Parallel Training of Deep Neural
Networks},
 url = {https://arxiv.org/abs/1606.00575v2},
 year = {2016}
}


@article{WzFOJBiA,
 abstract = {We study the problem of attributing the prediction of a deep network to its
input features, a problem previously studied by several other works. We
identify two fundamental axioms---Sensitivity and Implementation Invariance
that attribution methods ought to satisfy. We show that they are not satisfied
by most known attribution methods, which we consider to be a fundamental
weakness of those methods. We use the axioms to guide the design of a new
attribution method called Integrated Gradients. Our method requires no
modification to the original network and is extremely simple to implement; it
just needs a few calls to the standard gradient operator. We apply this method
to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
 archiveprefix = {arXiv},
 author = {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
 eprint = {1703.01365v2},
 file = {1703.01365v2.pdf},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {Axiomatic Attribution for Deep Networks},
 url = {https://arxiv.org/abs/1703.01365v2},
 year = {2017}
}


@article{2cMhMv5A,
 abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent
performance on difficult learning tasks. Although DNNs work well whenever large
labeled training sets are available, they cannot be used to map sequences to
sequences. In this paper, we present a general end-to-end approach to sequence
learning that makes minimal assumptions on the sequence structure. Our method
uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to
a vector of a fixed dimensionality, and then another deep LSTM to decode the
target sequence from the vector. Our main result is that on an English to
French translation task from the WMT'14 dataset, the translations produced by
the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's
BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did
not have difficulty on long sentences. For comparison, a phrase-based SMT
system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
BLEU score increases to 36.5, which is close to the previous best result on
this task. The LSTM also learned sensible phrase and sentence representations
that are sensitive to word order and are relatively invariant to the active and
the passive voice. Finally, we found that reversing the order of the words in
all source sentences (but not target sentences) improved the LSTM's performance
markedly, because doing so introduced many short term dependencies between the
source and the target sentence which made the optimization problem easier.},
 archiveprefix = {arXiv},
 author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
 eprint = {1409.3215v3},
 file = {1409.3215v3.pdf},
 month = {Sep},
 primaryclass = {cs.CL},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://arxiv.org/abs/1409.3215v3},
 year = {2014}
}


@article{hOeUlCvS,
 abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an
implementation for executing such algorithms. A computation expressed using
TensorFlow can be executed with little or no change on a wide variety of
heterogeneous systems, ranging from mobile devices such as phones and tablets
up to large-scale distributed systems of hundreds of machines and thousands of
computational devices such as GPU cards. The system is flexible and can be used
to express a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been used for conducting
research and for deploying machine learning systems into production across more
than a dozen areas of computer science and other fields, including speech
recognition, computer vision, robotics, information retrieval, natural language
processing, geographic information extraction, and computational drug
discovery. This paper describes the TensorFlow interface and an implementation
of that interface that we have built at Google. The TensorFlow API and a
reference implementation were released as an open-source package under the
Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
 archiveprefix = {arXiv},
 author = {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
 eprint = {1603.04467v2},
 file = {1603.04467v2.pdf},
 month = {Mar},
 primaryclass = {cs.DC},
 title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
Systems},
 url = {https://arxiv.org/abs/1603.04467v2},
 year = {2016}
}


@article{1GhHIDxuW,
 abstract = {We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.},
 archiveprefix = {arXiv},
 author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
 eprint = {1301.3781v3},
 file = {1301.3781v3.pdf},
 month = {Jan},
 primaryclass = {cs.CL},
 title = {Efficient Estimation of Word Representations in Vector Space},
 url = {https://arxiv.org/abs/1301.3781v3},
 year = {2013}
}


@article{16OPHvAij,
 abstract = {Molecular machine learning has been maturing rapidly over the last few years.
Improved methods and the presence of larger datasets have enabled machine
learning algorithms to make increasingly accurate predictions about molecular
properties. However, algorithmic progress has been limited due to the lack of a
standard benchmark to compare the efficacy of proposed methods; most new
algorithms are benchmarked on different datasets making it challenging to gauge
the quality of proposed methods. This work introduces MoleculeNet, a large
scale benchmark for molecular machine learning. MoleculeNet curates multiple
public datasets, establishes metrics for evaluation, and offers high quality
open-source implementations of multiple previously proposed molecular
featurization and learning algorithms (released as part of the DeepChem open
source library). MoleculeNet benchmarks demonstrate that learnable
representations, and in particular graph convolutional networks, are powerful
tools for molecular machine learning and broadly offer the best performance.
However, for quantum mechanical and biophysical datasets, the use of
physics-aware featurizations can be significantly more important than choice of
particular learning algorithm.},
 archiveprefix = {arXiv},
 author = {Zhenqin Wu and Bharath Ramsundar and Evan N. Feinberg and Joseph Gomes and Caleb Geniesse and Aneesh S. Pappu and Karl Leswing and Vijay Pande},
 eprint = {1703.00564v1},
 file = {1703.00564v1.pdf},
 month = {Mar},
 primaryclass = {cs.LG},
 title = {MoleculeNet: A Benchmark for Molecular Machine Learning},
 url = {https://arxiv.org/abs/1703.00564v1},
 year = {2017}
}


@article{yHn4SDRI,
 abstract = {Inspired by recent work in machine translation and object detection, we
introduce an attention based model that automatically learns to describe the
content of images. We describe how we can train this model in a deterministic
manner using standard backpropagation techniques and stochastically by
maximizing a variational lower bound. We also show through visualization how
the model is able to automatically learn to fix its gaze on salient objects
while generating the corresponding words in the output sequence. We validate
the use of attention with state-of-the-art performance on three benchmark
datasets: Flickr8k, Flickr30k and MS COCO.},
 archiveprefix = {arXiv},
 author = {Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio},
 eprint = {1502.03044v3},
 file = {1502.03044v3.pdf},
 month = {Feb},
 primaryclass = {cs.LG},
 title = {Show, Attend and Tell: Neural Image Caption Generation with Visual
Attention},
 url = {https://arxiv.org/abs/1502.03044v3},
 year = {2015}
}


@article{17i18PMkR,
 abstract = {Recent years have produced great advances in training large, deep neural
networks (DNNs), including notable successes in training convolutional neural
networks (convnets) to recognize natural images. However, our understanding of
how these models work, especially what computations they perform at
intermediate layers, has lagged behind. Progress in the field will be further
accelerated by the development of better tools for visualizing and interpreting
neural nets. We introduce two such tools here. The first is a tool that
visualizes the activations produced on each layer of a trained convnet as it
processes an image or video (e.g. a live webcam stream). We have found that
looking at live activations that change in response to user input helps build
valuable intuitions about how convnets work. The second tool enables
visualizing features at each layer of a DNN via regularized optimization in
image space. Because previous versions of this idea produced less recognizable
images, here we introduce several new regularization methods that combine to
produce qualitatively clearer, more interpretable visualizations. Both tools
are open source and work on a pre-trained convnet with minimal setup.},
 archiveprefix = {arXiv},
 author = {Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},
 eprint = {1506.06579v1},
 file = {1506.06579v1.pdf},
 month = {Jun},
 primaryclass = {cs.CV},
 title = {Understanding Neural Networks Through Deep Visualization},
 url = {https://arxiv.org/abs/1506.06579v1},
 year = {2015}
}


@article{voh0OiT2,
 abstract = {Large Convolutional Network models have recently demonstrated impressive
classification performance on the ImageNet benchmark. However there is no clear
understanding of why they perform so well, or how they might be improved. In
this paper we address both issues. We introduce a novel visualization technique
that gives insight into the function of intermediate feature layers and the
operation of the classifier. We also perform an ablation study to discover the
performance contribution from different model layers. This enables us to find
model architectures that outperform Krizhevsky \etal on the ImageNet
classification benchmark. We show our ImageNet model generalizes well to other
datasets: when the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
 archiveprefix = {arXiv},
 author = {Matthew D Zeiler and Rob Fergus},
 eprint = {1311.2901v3},
 file = {1311.2901v3.pdf},
 month = {Dec},
 primaryclass = {cs.CV},
 title = {Visualizing and Understanding Convolutional Networks},
 url = {https://arxiv.org/abs/1311.2901v3},
 year = {2013}
}


@article{Kk20paR7,
 abstract = {This article presents the prediction difference analysis method for
visualizing the response of a deep neural network to a specific input. When
classifying images, the method highlights areas in a given input image that
provide evidence for or against a certain class. It overcomes several
shortcoming of previous methods and provides great additional insight into the
decision making process of classifiers. Making neural network decisions
interpretable through visualization is important both to improve models and to
accelerate the adoption of black-box classifiers in application areas such as
medicine. We illustrate the method in experiments on natural images (ImageNet
data), as well as medical images (MRI brain scans).},
 archiveprefix = {arXiv},
 author = {Luisa M Zintgraf and Taco S Cohen and Tameem Adel and Max Welling},
 eprint = {1702.04595v1},
 file = {1702.04595v1.pdf},
 month = {Feb},
 primaryclass = {cs.CV},
 title = {Visualizing Deep Neural Network Decisions: Prediction Difference
Analysis},
 url = {https://arxiv.org/abs/1702.04595v1},
 year = {2017}
}

